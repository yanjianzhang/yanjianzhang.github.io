<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta name="google-site-verification" content="EnW3iE5uXnkCt08dH5N1p2UztpHHWUslmyE_mIIGaac"><meta name="baidu-site-verification" content="code-EtTckMomHC"><meta charset="UTF-8"><title>Good-Turing、Absolute、kneser-ney smooth</title><meta name="description" content="精诚致志，求仁存心"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Good-Turing smoothingGood-Turing基本思想是：用观察计数较高的N元语法数重新估计概率量的大小，并把它指派给那些具有零计数或者较低计数的N元语法。公式:
c^{×} = \frac{(c+1)N_{c+1}}{N_c}其中c为某个N-gram出现的频数, $N_c$ 为出现次数为c的N-Gram的词组的个数，$c^×$为Good-Turing平滑计数例子:对于a = [A,A,A,B,B,C,D,E] sum(len(a)) = 7$c_A = 3$    $c_C = 2$
$c_B = c_D = c_E = 1$
$N_1=4$ $N_2=1$ $N_3=1$ 
thus 
c^{×} = 4 × \frac{0}{3} = 0c^×_B = 3×\frac{1}{1} =.."><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Yanjian Zhang's Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Yanjian Zhang's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Good-Turing、Absolute、kneser-ney smooth</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Good-Turing-smoothing"><span class="toc-text">Good-Turing smoothing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Absolute-discounting"><span class="toc-text">Absolute discounting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kneser-Ney-smoothing"><span class="toc-text">Kneser-Ney smoothing</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/smooth"><i class="tag post-item-tag">smooth</i></a><a href="/tags/kneser-ney"><i class="tag post-item-tag">kneser-ney</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Good-Turing、Absolute、kneser-ney smooth</h1><time class="has-text-grey" datetime="2018-11-03T23:44:02.000Z">2018-11-04</time><article class="mt-2 post-content"><h3 id="Good-Turing-smoothing"><a href="#Good-Turing-smoothing" class="headerlink" title="Good-Turing smoothing"></a>Good-Turing smoothing</h3><p>Good-Turing基本思想是：用观察计数较高的N元语法数重新估计概率量的大小，并把它指派给那些具有零计数或者较低计数的N元语法。<br>公式:</p>
<script type="math/tex; mode=display">c^{×} = \frac{(c+1)N_{c+1}}{N_c}</script><p>其中c为某个N-gram出现的频数, $N_c$ 为出现次数为c的N-Gram的词组的个数，$c^×$为Good-Turing平滑计数<br>例子:对于a = [A,A,A,B,B,C,D,E] sum(len(a)) = 7<br>$c_A = 3$    $c_C = 2$</p>
<p>$c_B = c_D = c_E = 1$</p>
<p>$N_1=4$ $N_2=1$ $N_3=1$ </p>
<p>thus </p>
<script type="math/tex; mode=display">c^{×} = 4 × \frac{0}{3} = 0</script><script type="math/tex; mode=display">c^×_B = 3×\frac{1}{1} = 3</script><script type="math/tex; mode=display">c^×_C = c^×_D = c^×_E = 2×\frac{1}{4} = \frac{1}{2}</script><p>（注意在他们求概率的时候都需要除以N = 7）<br>然而这样会导致最高频率的结果最后的计数为0，解决方法如下</p>
<ol>
<li>可以对于较低的计数使用$P_{GT}$,而对于较高的计数直接使用$P_{MLE} = \frac{c}{N}$</li>
<li>也可以在较大的计数的时候使用$F(r) = ar^b $,其中a，b为参数，b&lt;-1</li>
</ol>
<p>最终会出现概率之和不为0的情况，这时候要进行归一化，固定没有见过的结果的概率，将已经见过的概率之和归一化使得共同的概率结果为1<br>归一化例子可见于<a target="_blank" rel="noopener" href="https://www.csd.uwo.ca/courses/CS4442b/L9-NLP-LangModels.pdf">L9-NLP-LangModels.pdf</a>第64页</p>
<h3 id="Absolute-discounting"><a href="#Absolute-discounting" class="headerlink" title="Absolute discounting"></a>Absolute discounting</h3><p>一般的interpolation是利用高阶的模型的$P_MLE$乘以一个$\lambda$,而此处是从每个非零计数中减掉一个固定的$\delta \in (0,1)$,一般取$\delta = 0.75$<br>在bigram中，公式为</p>
<script type="math/tex; mode=display">P_{AD}(w_i|w_{i-1}) = \frac{c(w_{i-1},w_i)-\delta}{c(w_{i-1})}+\lambda(w_{i-1})P(w))</script><p>其中$\lambda(w_{i-1})为插值权重系数$<br>于是完整的公式就是</p>
<script type="math/tex; mode=display">p_{abs}(w_i|w^i_{i-n+1}) = \frac{max\{c(w^i_{i-n+1})-\delta,0\}}{\sum_{w_i}c(w^i_{i-n+1})}+(1-\lambda_{w^{i-1}_{i-n+1}}p_{ab}(w_i|w^{i-1}{i-n+2}))</script><p>为了使得结果的总和为1</p>
<script type="math/tex; mode=display">1 - \lambda _ { w _ { i - n + 1 } } ^ { i - 1 } = \frac { \delta } { \sum _ { w _ { i } } c ( w _ { i - n + 1 } ^ { i } ) } N _ { 1 + } ( w _ { i - n + 1 } ^ { i - 1 }  \mathbf { \bullet } )</script><p>其中</p>
<script type="math/tex; mode=display">N _ { 1 + } \left( w _ { i - n + 1 } ^ { i - 1 }  \bullet \right) = \left| \left\{ w _ { i } : c \left( w _ { i - n + 1 } ^ { i - 1 } w _ { i } \right) > 0 \right\} \right|</script><h3 id="Kneser-Ney-smoothing"><a href="#Kneser-Ney-smoothing" class="headerlink" title="Kneser-Ney smoothing"></a>Kneser-Ney smoothing</h3><p>bigram下的公式:</p>
<script type="math/tex; mode=display">p _ { K N } \left( w _ { i } | w _ { i - 1 } \right) = \frac { \max \left( c \left( w _ { i - 1 } , w _ { i } \right) - \delta , 0 \right) } { \sum _ { w ^ { \prime } } c \left( w _ { i - 1 } , w ^ { \prime } \right) } + \lambda _ { w _ { i - 1 } } p _ { K N } \left( w _ { i } \right)</script><p>其中</p>
<script type="math/tex; mode=display">p _ { K N } \left( w _ { i } \right) = \frac { \left| \left\{ w ^ { \prime } : 0 < c \left( w ^ { \prime } , w _ { i } \right) \right\} \right| } { \left| \left\{ \left( w ^ { \prime } , w ^ { \prime \prime } \right) : 0 < c \left( w ^ { \prime } , w ^ { \prime \prime } \right) \right\} \right| }</script><p>为的是求解在一个不熟悉的上下文中看见单词$w_i$的可能性,这使用$w_i$在出现在所有单词的次数和除以所有bigram的和来衡量<br>减掉一个固定的$\delta \in (0,1)$,一般取$\delta = 0.75$<br>$\lambda_{w_{i-1}}$是用来平衡使得条件概率$p _ { K N } ( w _ { i } | w _ { i - 1 } )$的总和为1的系数<br>得出满足条件的$\lambda_{w_{i-1}}$结果为</p>
<script type="math/tex; mode=display">\lambda _ { w _ { i - 1 } } = \frac { \delta } { \sum _ { w ^ { \prime } } c \left( w _ { i - 1 } , w ^ { \prime } \right) } \left| \left\{ w ^ { \prime } : 0 < c \left( w _ { i - 1 } , w ^ { \prime } \right) \right\} \right|</script><p>可以推广到n-gram</p>
<script type="math/tex; mode=display">p _ { K N } \left( w _ { i } | w _ { i - n + 1 } ^ { i - 1 } \right) = \frac { \max \left( c \left( w _ { i - n + 1 } ^ { i - 1 } , w _ { i } \right) - \delta , 0 \right) } { \sum _ { w ^ { \prime } } c \left( w _ { i - n + 1 } ^ { i - 1 } , w ^ { \prime } \right) } + \delta \frac { \left| \left\{ w ^ { \prime } : 0 < c \left( w _ { i - n + 1 } ^ { i - 1 } , w ^ { \prime } \right) \right\} \right| } { \sum _ { w _ { i } } c \left( w _ { i - n + 1 } ^ { i } \right) } p _ { K N } \left( w _ { i } | w _ { i - n + 2 } ^ { i - 1 } \right)</script></article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2018/11/07/cookbookString/" title="字符串和文本处理"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: 字符串和文本处理</span></a><a class="button is-default" href="/2018/11/04/trie/" title="Trie树"><span class="has-text-weight-semibold">Next: Trie树</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/yanjianzhang"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/zhangyanjian"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><a title="linkedin" target="_blank" rel="noopener nofollow" href="//www.linkedin.com/in/yanjianzhang"><i class="iconfont icon-linkedin"></i></a><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com/zhang.yanjian.7"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Yanjian Zhang 2022</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>