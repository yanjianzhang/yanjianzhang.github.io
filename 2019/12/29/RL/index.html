<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta name="google-site-verification" content="EnW3iE5uXnkCt08dH5N1p2UztpHHWUslmyE_mIIGaac"><meta charset="UTF-8"><title>强化学习精要总结（基础）</title><meta name="description" content="精诚致志，求仁存心"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="本博文假设基本的状态表示和动作表示都已经了解的情况下，对多个强化学习方法进行简要对比
阅读长篇英文博文请点击这里
Major division: Value iteration and Policy iteration
1. Value IterationDynamic Programming: when the model is fully known, it can be directly solved by Dynamic Programming.
When the model is not solve, we need reinforcement learning.
值迭代就是顺着动态规划的思路，只要我们将值估计出来，就能够进行规划了。
Generalized Policy Iteration (G.."><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Yanjian Zhang's Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Yanjian Zhang's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">强化学习精要总结（基础）</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Value-Iteration"><span class="toc-text">1. Value Iteration</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Generalized-Policy-Iteration-GPI"><span class="toc-text">Generalized Policy Iteration (GPI)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Monte-Carlo-Methods"><span class="toc-text">Monte-Carlo Methods</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Temporal-Difference-Learning"><span class="toc-text">Temporal-Difference Learning:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SARSA-On-Policy-TD-control"><span class="toc-text">SARSA: On-Policy TD control</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Q-Learning-Off-policy-TD-control"><span class="toc-text">Q-Learning: Off-policy TD control</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Policy-Iteration"><span class="toc-text">2. Policy Iteration</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#REINFORCE"><span class="toc-text">REINFORCE</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Actor-Critic%EF%BC%9A%E7%BB%93%E5%90%88%E4%B8%A4%E9%83%A8%E5%88%86"><span class="toc-text">3. Actor-Critic：结合两部分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Asynchronous-Advantage-Actor-Critic-A3C"><span class="toc-text">Asynchronous Advantage Actor-Critic (A3C)</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Reinforcement%20Learning"><i class="tag post-item-tag">Reinforcement Learning</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">强化学习精要总结（基础）</h1><time class="has-text-grey" datetime="2019-12-28T23:00:00.000Z">2019-12-29</time><article class="mt-2 post-content"><p>本博文假设基本的状态表示和动作表示都已经了解的情况下，对多个强化学习方法进行简要对比</p>
<p>阅读长篇英文博文请<a target="_blank" rel="noopener" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">点击这里</a></p>
<p>Major division: <strong>Value iteration</strong> and <strong>Policy iteration</strong></p>
<h3 id="1-Value-Iteration"><a href="#1-Value-Iteration" class="headerlink" title="1. Value Iteration"></a>1. Value Iteration</h3><p>Dynamic Programming: when the model is fully known, it can be directly solved by Dynamic Programming.</p>
<p>When the model is not solve, we need reinforcement learning.</p>
<p>值迭代就是顺着动态规划的思路，只要我们将值估计出来，就能够进行规划了。</p>
<h4 id="Generalized-Policy-Iteration-GPI"><a href="#Generalized-Policy-Iteration-GPI" class="headerlink" title="Generalized Policy Iteration (GPI)"></a>Generalized Policy Iteration (GPI)</h4><p>$ V_{t+1}(s)=\mathbb{E}_{\pi}\left[r+\gamma V_{t}\left(s^{\prime}\right) | S_{t}=s\right]=\sum_{a} \pi(a | s) \sum_{s^{\prime}, r} P\left(s^{\prime}, r | s, a\right)\left(r+\gamma V_{k}\left(s^{\prime}\right)\right) $</p>
<p>$Q_{\pi}(s, a)=\mathbb{E}\left[R_{t+1}+\gamma V_{\pi}\left(S_{t+1}\right) | S_{t}=s, A_{t}=a\right]=\sum_{s^{\prime}, r} P\left(s^{\prime}, r | s, a\right)\left(r+\gamma V_{\pi}\left(s^{\prime}\right)\right)$</p>
<h4 id="Monte-Carlo-Methods"><a href="#Monte-Carlo-Methods" class="headerlink" title="Monte-Carlo Methods"></a>Monte-Carlo Methods</h4><p>使用采样出来的轨迹模拟上述公式<br>$V(s)=\frac{\sum_{t=1}^{T} 1\left[S_{t}=s\right] G_{t}}{\sum_{t=1}^{T} 1\left[S_{t}=s\right]}$     $Q(s, a)=\frac{\sum_{t=1}^{T} 1\left[S_{t}=s, A_{t}=a\right] G_{t}}{\sum_{t=1}^{T} 1\left[S_{t}=s, A_{t}=a\right]}$</p>
<h4 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal-Difference Learning:"></a>Temporal-Difference Learning:</h4><p>估计量改为 $R_{t+1}+\gamma V\left(S_{t+1}\right)$，即可得到<br>$V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)$<br>$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right)$</p>
<p>以下的方法皆为TD Learning的拓展：</p>
<h4 id="SARSA-On-Policy-TD-control"><a href="#SARSA-On-Policy-TD-control" class="headerlink" title="SARSA: On-Policy TD control"></a>SARSA: On-Policy TD control</h4><p>使用$\varepsilon$ -greedy来获得动作，从而得到两次动作（行动策略$a, s_t$ 和 评估策略$a’,s_{t+1}’$）<br>$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right)$</p>
<h4 id="Q-Learning-Off-policy-TD-control"><a href="#Q-Learning-Off-policy-TD-control" class="headerlink" title="Q-Learning: Off-policy TD control"></a>Q-Learning: Off-policy TD control</h4><p>Off-policy 是指行动策略和评估策略不同，Q-learning的评估策略是贪婪的，公式对比极易看出<br>$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t+1}+\gamma \max _{a \in \mathcal{A}} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right)_{-}$</p>
<h3 id="2-Policy-Iteration"><a href="#2-Policy-Iteration" class="headerlink" title="2. Policy Iteration"></a>2. Policy Iteration</h3><p>Policy Iteration 完全是另一种解决方案，是去优化策略值函数的期望<br>$\mathcal{J}(\theta)=\sum_{s \in \mathcal{S}} d_{\pi_{\theta}}(s) V_{\pi_{\theta}}(s)=\sum_{s \in \mathcal{S}}\left(d_{\pi_{\theta}}(s) \sum_{a \in \mathcal{A}} \pi(a | s, \theta) Q_{\pi}(s, a)\right)$</p>
<p>求梯度，并转化为$\pi_{\theta}$下的期望</p>
<p>$\nabla \mathcal{J}(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla \ln \pi(a | s, \theta) Q_{\pi}(s, a)\right]$</p>
<p>剩下的就是梯度更新的问题了</p>
<h4 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h4><p>使用蒙特卡洛的方法获得一系列轨迹，使用$A(s, a)=Q(s, a)-V(s)$来进行梯度更新<br>$\theta \leftarrow \theta+\alpha \gamma^{t} G_{t} \nabla \ln \pi\left(A_{t} | S_{t}, \theta\right)$</p>
<h3 id="3-Actor-Critic：结合两部分"><a href="#3-Actor-Critic：结合两部分" class="headerlink" title="3. Actor-Critic：结合两部分"></a>3. Actor-Critic：结合两部分</h3><p>既更新策略期望，又更新值函数<br>$\theta \leftarrow \theta+\alpha_{\theta} Q(s, a ; w) \nabla_{\theta} \ln \pi(a | s ; \theta)$<br>$w \leftarrow w+\alpha_{w} G_{t: t+1} \nabla_{w} Q(s, a ; w)$</p>
<h4 id="Asynchronous-Advantage-Actor-Critic-A3C"><a href="#Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="Asynchronous Advantage Actor-Critic (A3C)"></a>Asynchronous Advantage Actor-Critic (A3C)</h4><p>异步：多线程 优势：使用优势函数进行更新<br>$d \theta \leftarrow d \theta+\nabla_{\theta^{\prime}} \log \pi\left(a_{i} | s_{i} ; \theta^{\prime}\right)\left(R-V\left(s_{i} ; w^{\prime}\right)\right)$<br>$d w \leftarrow d w+\nabla_{w^{\prime}}\left(R-V\left(s_{i} ; w^{\prime}\right)\right)^{2}$<br>异步获得梯度，进行加和，同步更新</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2020/02/11/RLSeq2Seq/" title="NLG里的强化与对抗"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: NLG里的强化与对抗</span></a><a class="button is-default" href="/2019/12/27/doctor/" title="Top essays by scientists in 2019"><span class="has-text-weight-semibold">Next: Top essays by scientists in 2019</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/dongfanker"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/zhangyanjian"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><a title="linkedin" target="_blank" rel="noopener nofollow" href="//www.linkedin.com/in/yanjian-zhang-154975154"><i class="iconfont icon-linkedin"></i></a><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com/zhang.yanjian.7"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Yanjian Zhang 2021</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>