<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta name="google-site-verification" content="EnW3iE5uXnkCt08dH5N1p2UztpHHWUslmyE_mIIGaac"><meta name="baidu-site-verification" content="code-EtTckMomHC"><meta charset="UTF-8"><title>NLG里的强化与对抗</title><meta name="description" content="精诚致志，求仁存心"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="本篇讲述三篇论文
《SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient》
《Adversarial learning for neural dialogue generation》
《Deep Reinforcement Learning for Sequence-to-Sequence Models》
对于文本生成来说一般会有编码器(可以来源于文本，图像和图网络信息)，还会有解码器
解码器的loss往往是使用每个词的NLL(negative log likelihood)来获得，而这意味着一旦进行了解码，就没有办法对整句话进行评价了。因此为了解决这个问题，往往使用强化学习的方法。
SeqGANs就使用了强化学习的方法来实现反向.."><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Yanjian Zhang's Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Yanjian Zhang's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">NLG里的强化与对抗</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile is-hidden"></div><div class="column is-9"><header class="my-4"><a href="/tags/Natural%20Language%20Processing"><i class="tag post-item-tag">Natural Language Processing</i></a><a href="/tags/Reinforcement%20Learning"><i class="tag post-item-tag">Reinforcement Learning</i></a><a href="/tags/Adversarial%20Learning"><i class="tag post-item-tag">Adversarial Learning</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">NLG里的强化与对抗</h1><time class="has-text-grey" datetime="2020-02-10T23:00:00.000Z">2020-02-11</time><article class="mt-2 post-content"><p>本篇讲述三篇论文</p>
<p>《SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient》</p>
<p>《Adversarial learning for neural dialogue generation》</p>
<p>《Deep Reinforcement Learning for Sequence-to-Sequence Models》</p>
<p>对于文本生成来说一般会有编码器(可以来源于文本，图像和图网络信息)，还会有解码器</p>
<p>解码器的loss往往是使用每个词的NLL(negative log likelihood)来获得，而这意味着一旦进行了解码，就没有办法对整句话进行评价了。因此为了解决这个问题，往往使用强化学习的方法。</p>
<p>SeqGANs就使用了强化学习的方法来实现反向传播。中间为了使得生成完整的句子用于Discriminator能够衡量reward，还使用了蒙特卡洛的方法。之所以使用MCTS这主要受限于问题的假设，是没有编码器的文本生成，一开始的生成序列会非常短。这意味着就是生成的结果基本取决于训练的文本，就像是图像中的的人脸数据集就会生成人脸，而油画数据集就会生成油画一样。</p>
<p>对于我来说，最难的部分就在于如何在代码中计算loss，我在SeqGANs的代码中看到这样一段</p>
<pre><code class="lang-python">def batchPGLoss(self, inp, target, reward):
        &quot;&quot;&quot;
        Returns a pseudo-loss that gives corresponding policy gradients (on calling .backward()).
        Inspired by the example in http://karpathy.github.io/2016/05/31/rl/
        Inputs: inp, target
            - inp: batch_size x seq_len
            - target: batch_size x seq_len
            - reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding
                      sentence)
            inp should be target with &lt;s&gt; (start letter) prepended
        &quot;&quot;&quot;

        batch_size, seq_len = inp.size()
        inp = inp.permute(1, 0)          # seq_len x batch_size
        target = target.permute(1, 0)    # seq_len x batch_size
        h = self.init_hidden(batch_size)

        loss = 0
        for i in range(seq_len):
            out, h = self.forward(inp[i], h)
            # TODO: should h be detached from graph (.detach())?
            for j in range(batch_size):
                loss += -out[j][target.data[i][j]]*reward[j]     # log(P(y_t|Y_1:Y_&#123;t-1&#125;)) * Q

        return loss/batch_size
</code></pre>
<p>访问代码中的博客<a target="_blank" rel="noopener" href="http://karpathy.github.io/2016/05/31/rl/，">http://karpathy.github.io/2016/05/31/rl/，</a> 我确实被惊艳了，作者正好说了我最需要关注的东西，监督学习与强化学习的比较，我确实不敢相信它是如此的2016 (just joking)。</p>
<p>不过蒙特卡洛除了解决这一个问题之外，还解决了一个Reward for Every Generation Step(REGS)的问题，以下是从<a target="_blank" rel="noopener" href="https://www.cnblogs.com/n2meetu/p/8182194.html">其他博文</a>中找到的描述</p>
<p>‘’在以往的工作中，D效果非常好而G的效果非常糟糕会带来训练效果的下降。试想一下一个G所有产生的答案都被D驳回了，在这段时间内G的所有反馈都是负反馈，G就会迷失从而不知道向什么方向优化会得到正反馈，所以理想的情况下G和D是交替训练上升的。’’</p>
<p>《Adversarial learning for neural dialogue generation》正是注意到了这一点，对其使用部分片段的评价进行优化。</p>
<p>之后的内容待以后再更新….</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2020/02/19/NLP_toolkit/" title="常见NLP工具包"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: 常见NLP工具包</span></a><a class="button is-default" href="/2019/12/29/RL/" title="强化学习精要总结（基础）"><span class="has-text-weight-semibold">Next: 强化学习精要总结（基础）</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><script src="/js/post.js"></script></body></html>