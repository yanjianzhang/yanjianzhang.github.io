<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yanjian Zhang&#39;s Blog</title>
  
  
  <link href="https://yanjianzhang.github.io/atom.xml" rel="self"/>
  
  <link href="https://yanjianzhang.github.io/"/>
  <updated>2022-09-16T09:21:33.673Z</updated>
  <id>https://yanjianzhang.github.io/</id>
  
  <author>
    <name>Yanjian Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pre-order traversal and Recursive</title>
    <link href="https://yanjianzhang.github.io/2022/09/16/recursive_20220916/"/>
    <id>https://yanjianzhang.github.io/2022/09/16/recursive_20220916/</id>
    <published>2022-09-15T22:00:00.000Z</published>
    <updated>2022-09-16T09:21:33.673Z</updated>
    
    <content type="html"><![CDATA[<p>今天重新回顾了深度优先搜索和递归，<br>突然发现递归过程可以看成是对于程序数的先序遍历,中序遍历和后序遍历<br>比方说以下例子</p><pre><code class="lang-python">def DFS()    print(1)        # node itself    DFS()           # left tree    DFS()           # right tree</code></pre><p>以上例子就可以看成先序遍历的二叉树</p><pre><code class="lang-python">def DFS()    DFS()           # left tree    print(1)        # node itself    DFS()           # right tree</code></pre><p>以上例子就可以看成中序遍历的二叉树</p><p>同理还可以有多叉树等等，用这样的思路思考递归运算的程序复杂度就可以参考遍历树的复杂度来求解了</p>]]></content>
    
    
    <summary type="html">先序遍历与递归调用的关系</summary>
    
    
    
    <category term="Programming" scheme="https://yanjianzhang.github.io/categories/Programming/"/>
    
    
    <category term="Programming" scheme="https://yanjianzhang.github.io/tags/Programming/"/>
    
  </entry>
  
  <entry>
    <title>常见NLP工具包</title>
    <link href="https://yanjianzhang.github.io/2020/02/18/NLP_toolkit/"/>
    <id>https://yanjianzhang.github.io/2020/02/18/NLP_toolkit/</id>
    <published>2020-02-18T16:00:00.000Z</published>
    <updated>2020-02-19T08:58:44.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="FudanNLP-FNLP"><a href="#FudanNLP-FNLP" class="headerlink" title="FudanNLP (FNLP)"></a>FudanNLP (FNLP)</h4><p><a href="https://github.com/FudanNLP/fnlp">Github 地址</a></p><p>主要是为中文自然语言处理而开发的工具包，也包含为实现这些任务的机器学习算法和数据集。</p><h4 id="fastNLP"><a href="#fastNLP" class="headerlink" title="fastNLP"></a>fastNLP</h4><p><a href="https://github.com/fastnlp/fastNLP">Github 地址</a></p><p>中文的NLP工具包，提供多种神经网络组件以及复现模型（涵盖中文分词、命名实体识别、句法分析、文本分类、文本匹配、指代消解、摘要等任务）;</p><h4 id="Stanford-CoreNLP"><a href="#Stanford-CoreNLP" class="headerlink" title="Stanford CoreNLP"></a>Stanford CoreNLP</h4><p><a href="https://github.com/pytorch/fairseq">主页地址</a><br>是基于java的程序包，提供一系列 jar 包用于命名实体识别(NER)、共指消解(Coreference)、依赖分析</p><h4 id="StanfordNLP"><a href="#StanfordNLP" class="headerlink" title="StanfordNLP"></a>StanfordNLP</h4><p><a href="https://github.com/stanfordnlp/stanfordnlp">Github 地址</a></p><p>主要用于词法特征标记和依赖项解析，比如词性标注(POS)，词元分析（Lemma）, 依赖分析（dependency relation）</p><h4 id="Pytorch-NLP"><a href="#Pytorch-NLP" class="headerlink" title="Pytorch-NLP"></a>Pytorch-NLP</h4><p><a href="https://github.com/PetrochukM/PyTorch-NLP">Github 地址</a></p><p>个人比较文本预处理工具包，可以很快将自己的新数据集转化为可以用于训练的batch，引入词向量也很方便。</p><h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><p><a href="https://github.com/huggingface/transformers">Github 地址</a> </p><p>能够直接展示自动补全的writing是其一大特色, 可见他们的<a href="https://transformer.huggingface.co/">Online demo</a></p><h3 id="Neural-machine-translation"><a href="#Neural-machine-translation" class="headerlink" title="Neural machine translation"></a>Neural machine translation</h3><h4 id="OpenNMT"><a href="#OpenNMT" class="headerlink" title="OpenNMT"></a>OpenNMT</h4><p><a href="https://github.com/OpenNMT/OpenNMT-py">Github pytorch 地址</a>  <a href="https://github.com/OpenNMT/OpenNMT-tf">Github tensorflow 地址</a></p><p>个人感觉非常好用，不过pytorch的版本需要pytorch1.12，非CUDA 9.2以上的版本需要自己编译pytorch。</p><p>pytorch文档中直接提供了Seq2Seq以及transformer的直接使用方法【<a href="http://opennmt.net/OpenNMT-py/extended.html">Seq2Seq</a> <a href="http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model">Transformer</a>】，同时还提供Image2Text，Speech2Text以及Video2Text的使用方法</p><p>tensorflow的使用方法也很直接。</p><h4 id="FairSeq"><a href="#FairSeq" class="headerlink" title="FairSeq"></a>FairSeq</h4><p><a href="https://github.com/pytorch/fairseq">Github 地址</a></p><p>Facebook AI Research 专门为 Torch定制的翻译模型，后面也开源了pytorch版本</p><p>基于卷积神经网络的翻译模型首先是Facebook提出来的，所以在这个工具包里面是满满的卷积网络</p><h4 id="UniLM"><a href="#UniLM" class="headerlink" title="UniLM"></a>UniLM</h4><p><a href="https://github.com/microsoft/unilm">Github 地址</a></p><p>当前最强的seq-to-seq的语言模型</p>]]></content>
    
    
    <summary type="html">Brief introduction to common NLP toolkits</summary>
    
    
    
    <category term="Natural Language Processing" scheme="https://yanjianzhang.github.io/categories/Natural-Language-Processing/"/>
    
    
    <category term="Natural Language Processing" scheme="https://yanjianzhang.github.io/tags/Natural-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title>jupyter notebook 中配置 conda python kernel</title>
    <link href="https://yanjianzhang.github.io/2020/02/18/ipykernel/"/>
    <id>https://yanjianzhang.github.io/2020/02/18/ipykernel/</id>
    <published>2020-02-18T16:00:00.000Z</published>
    <updated>2020-02-19T07:40:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>Conda 环境配置</p><pre><code class="lang-shell">conda create --name &lt;env_name&gt; &lt;package_name&gt;conda create --name tf python=3.6source activate tf</code></pre><p>Conda 环境中安装 jupyter notebook 内核</p><pre><code class="lang-shell">conda install nb_condapip install ipykernelpython -m ipykernel install --user --name tf --display-name tf</code></pre>]]></content>
    
    
    <summary type="html">Conda python in jupyter notebook</summary>
    
    
    
    <category term="jupyter notebook" scheme="https://yanjianzhang.github.io/categories/jupyter-notebook/"/>
    
    
    <category term="jupyter notebook" scheme="https://yanjianzhang.github.io/tags/jupyter-notebook/"/>
    
    <category term="Conda" scheme="https://yanjianzhang.github.io/tags/Conda/"/>
    
  </entry>
  
  <entry>
    <title>NLG里的强化与对抗</title>
    <link href="https://yanjianzhang.github.io/2020/02/10/RLSeq2Seq/"/>
    <id>https://yanjianzhang.github.io/2020/02/10/RLSeq2Seq/</id>
    <published>2020-02-10T16:00:00.000Z</published>
    <updated>2021-12-26T22:45:08.612Z</updated>
    
    <content type="html"><![CDATA[<p>本篇讲述三篇论文</p><p>《SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient》</p><p>《Adversarial learning for neural dialogue generation》</p><p>《Deep Reinforcement Learning for Sequence-to-Sequence Models》</p><p>对于文本生成来说一般会有编码器(可以来源于文本，图像和图网络信息)，还会有解码器</p><p>解码器的loss往往是使用每个词的NLL(negative log likelihood)来获得，而这意味着一旦进行了解码，就没有办法对整句话进行评价了。因此为了解决这个问题，往往使用强化学习的方法。</p><p>SeqGANs就使用了强化学习的方法来实现反向传播。中间为了使得生成完整的句子用于Discriminator能够衡量reward，还使用了蒙特卡洛的方法。之所以使用MCTS这主要受限于问题的假设，是没有编码器的文本生成，一开始的生成序列会非常短。这意味着就是生成的结果基本取决于训练的文本，就像是图像中的的人脸数据集就会生成人脸，而油画数据集就会生成油画一样。</p><p>对于我来说，最难的部分就在于如何在代码中计算loss，我在SeqGANs的代码中看到这样一段</p><pre><code class="lang-python">def batchPGLoss(self, inp, target, reward):        &quot;&quot;&quot;        Returns a pseudo-loss that gives corresponding policy gradients (on calling .backward()).        Inspired by the example in http://karpathy.github.io/2016/05/31/rl/        Inputs: inp, target            - inp: batch_size x seq_len            - target: batch_size x seq_len            - reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding                      sentence)            inp should be target with &lt;s&gt; (start letter) prepended        &quot;&quot;&quot;        batch_size, seq_len = inp.size()        inp = inp.permute(1, 0)          # seq_len x batch_size        target = target.permute(1, 0)    # seq_len x batch_size        h = self.init_hidden(batch_size)        loss = 0        for i in range(seq_len):            out, h = self.forward(inp[i], h)            # TODO: should h be detached from graph (.detach())?            for j in range(batch_size):                loss += -out[j][target.data[i][j]]*reward[j]     # log(P(y_t|Y_1:Y_&#123;t-1&#125;)) * Q        return loss/batch_size</code></pre><p>访问代码中的博客<a href="http://karpathy.github.io/2016/05/31/rl/，">http://karpathy.github.io/2016/05/31/rl/，</a> 我确实被惊艳了，作者正好说了我最需要关注的东西，监督学习与强化学习的比较，我确实不敢相信它是如此的2016 (just joking)。</p><p>不过蒙特卡洛除了解决这一个问题之外，还解决了一个Reward for Every Generation Step(REGS)的问题，以下是从<a href="https://www.cnblogs.com/n2meetu/p/8182194.html">其他博文</a>中找到的描述</p><p>‘’在以往的工作中，D效果非常好而G的效果非常糟糕会带来训练效果的下降。试想一下一个G所有产生的答案都被D驳回了，在这段时间内G的所有反馈都是负反馈，G就会迷失从而不知道向什么方向优化会得到正反馈，所以理想的情况下G和D是交替训练上升的。’’</p><p>《Adversarial learning for neural dialogue generation》正是注意到了这一点，对其使用部分片段的评价进行优化。</p><p>之后的内容待以后再更新….</p>]]></content>
    
    
    <summary type="html">Reinforcement and Adversarial Learning in Natural Language Generation</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="https://yanjianzhang.github.io/categories/Reinforcement-Learning/"/>
    
    
    <category term="Natural Language Processing" scheme="https://yanjianzhang.github.io/tags/Natural-Language-Processing/"/>
    
    <category term="Reinforcement Learning" scheme="https://yanjianzhang.github.io/tags/Reinforcement-Learning/"/>
    
    <category term="Adversarial Learning" scheme="https://yanjianzhang.github.io/tags/Adversarial-Learning/"/>
    
  </entry>
  
  <entry>
    <title>强化学习精要总结（基础）</title>
    <link href="https://yanjianzhang.github.io/2019/12/28/RL/"/>
    <id>https://yanjianzhang.github.io/2019/12/28/RL/</id>
    <published>2019-12-28T16:00:00.000Z</published>
    <updated>2020-02-19T07:41:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>本博文假设基本的状态表示和动作表示都已经了解的情况下，对多个强化学习方法进行简要对比</p><p>阅读长篇英文博文请<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">点击这里</a></p><p>Major division: <strong>Value iteration</strong> and <strong>Policy iteration</strong></p><h3 id="1-Value-Iteration"><a href="#1-Value-Iteration" class="headerlink" title="1. Value Iteration"></a>1. Value Iteration</h3><p>Dynamic Programming: when the model is fully known, it can be directly solved by Dynamic Programming.</p><p>When the model is not solve, we need reinforcement learning.</p><p>值迭代就是顺着动态规划的思路，只要我们将值估计出来，就能够进行规划了。</p><h4 id="Generalized-Policy-Iteration-GPI"><a href="#Generalized-Policy-Iteration-GPI" class="headerlink" title="Generalized Policy Iteration (GPI)"></a>Generalized Policy Iteration (GPI)</h4><p>$ V_{t+1}(s)=\mathbb{E}_{\pi}\left[r+\gamma V_{t}\left(s^{\prime}\right) | S_{t}=s\right]=\sum_{a} \pi(a | s) \sum_{s^{\prime}, r} P\left(s^{\prime}, r | s, a\right)\left(r+\gamma V_{k}\left(s^{\prime}\right)\right) $</p><p>$Q_{\pi}(s, a)=\mathbb{E}\left[R_{t+1}+\gamma V_{\pi}\left(S_{t+1}\right) | S_{t}=s, A_{t}=a\right]=\sum_{s^{\prime}, r} P\left(s^{\prime}, r | s, a\right)\left(r+\gamma V_{\pi}\left(s^{\prime}\right)\right)$</p><h4 id="Monte-Carlo-Methods"><a href="#Monte-Carlo-Methods" class="headerlink" title="Monte-Carlo Methods"></a>Monte-Carlo Methods</h4><p>使用采样出来的轨迹模拟上述公式<br>$V(s)=\frac{\sum_{t=1}^{T} 1\left[S_{t}=s\right] G_{t}}{\sum_{t=1}^{T} 1\left[S_{t}=s\right]}$     $Q(s, a)=\frac{\sum_{t=1}^{T} 1\left[S_{t}=s, A_{t}=a\right] G_{t}}{\sum_{t=1}^{T} 1\left[S_{t}=s, A_{t}=a\right]}$</p><h4 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal-Difference Learning:"></a>Temporal-Difference Learning:</h4><p>估计量改为 $R_{t+1}+\gamma V\left(S_{t+1}\right)$，即可得到<br>$V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)$<br>$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right)$</p><p>以下的方法皆为TD Learning的拓展：</p><h4 id="SARSA-On-Policy-TD-control"><a href="#SARSA-On-Policy-TD-control" class="headerlink" title="SARSA: On-Policy TD control"></a>SARSA: On-Policy TD control</h4><p>使用$\varepsilon$ -greedy来获得动作，从而得到两次动作（行动策略$a, s_t$ 和 评估策略$a’,s_{t+1}’$）<br>$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right)$</p><h4 id="Q-Learning-Off-policy-TD-control"><a href="#Q-Learning-Off-policy-TD-control" class="headerlink" title="Q-Learning: Off-policy TD control"></a>Q-Learning: Off-policy TD control</h4><p>Off-policy 是指行动策略和评估策略不同，Q-learning的评估策略是贪婪的，公式对比极易看出<br>$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t+1}+\gamma \max _{a \in \mathcal{A}} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right)_{-}$</p><h3 id="2-Policy-Iteration"><a href="#2-Policy-Iteration" class="headerlink" title="2. Policy Iteration"></a>2. Policy Iteration</h3><p>Policy Iteration 完全是另一种解决方案，是去优化策略值函数的期望<br>$\mathcal{J}(\theta)=\sum_{s \in \mathcal{S}} d_{\pi_{\theta}}(s) V_{\pi_{\theta}}(s)=\sum_{s \in \mathcal{S}}\left(d_{\pi_{\theta}}(s) \sum_{a \in \mathcal{A}} \pi(a | s, \theta) Q_{\pi}(s, a)\right)$</p><p>求梯度，并转化为$\pi_{\theta}$下的期望</p><p>$\nabla \mathcal{J}(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla \ln \pi(a | s, \theta) Q_{\pi}(s, a)\right]$</p><p>剩下的就是梯度更新的问题了</p><h4 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h4><p>使用蒙特卡洛的方法获得一系列轨迹，使用$A(s, a)=Q(s, a)-V(s)$来进行梯度更新<br>$\theta \leftarrow \theta+\alpha \gamma^{t} G_{t} \nabla \ln \pi\left(A_{t} | S_{t}, \theta\right)$</p><h3 id="3-Actor-Critic：结合两部分"><a href="#3-Actor-Critic：结合两部分" class="headerlink" title="3. Actor-Critic：结合两部分"></a>3. Actor-Critic：结合两部分</h3><p>既更新策略期望，又更新值函数<br>$\theta \leftarrow \theta+\alpha_{\theta} Q(s, a ; w) \nabla_{\theta} \ln \pi(a | s ; \theta)$<br>$w \leftarrow w+\alpha_{w} G_{t: t+1} \nabla_{w} Q(s, a ; w)$</p><h4 id="Asynchronous-Advantage-Actor-Critic-A3C"><a href="#Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="Asynchronous Advantage Actor-Critic (A3C)"></a>Asynchronous Advantage Actor-Critic (A3C)</h4><p>异步：多线程 优势：使用优势函数进行更新<br>$d \theta \leftarrow d \theta+\nabla_{\theta^{\prime}} \log \pi\left(a_{i} | s_{i} ; \theta^{\prime}\right)\left(R-V\left(s_{i} ; w^{\prime}\right)\right)$<br>$d w \leftarrow d w+\nabla_{w^{\prime}}\left(R-V\left(s_{i} ; w^{\prime}\right)\right)^{2}$<br>异步获得梯度，进行加和，同步更新</p>]]></content>
    
    
    <summary type="html">Short Note, the comparison and development of RL methods</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="https://yanjianzhang.github.io/categories/Reinforcement-Learning/"/>
    
    
    <category term="Reinforcement Learning" scheme="https://yanjianzhang.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Top essays by scientists in 2019</title>
    <link href="https://yanjianzhang.github.io/2019/12/26/doctor/"/>
    <id>https://yanjianzhang.github.io/2019/12/26/doctor/</id>
    <published>2019-12-26T16:00:00.000Z</published>
    <updated>2019-12-27T06:00:36.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.sciencemag.org/careers/2019/12/our-top-essays-scientists-2019">Source Link</a></p><p>Ashley Stenzel’s essay about the joys and challenges of raising two daughters while working toward a Ph.D. struck a chord with readers. “Good to know I’m not alone in this struggle. The guilt can be horrible,” one reader wrote on Science’s Facebook page earlier this month. “I loved every word of the story. As a graduate student with a child I can totally relate,” wrote another.<br>Stenzel’s piece was one of 51 essays we published this year as part of our ongoing Working Life series, which shines a spotlight on personal and professional challenges that scientists face as they pursue their careers. As the year draws to a close, we put together a list of our 10 most popular essays of 2019. Read on to find out about a disastrous postdoc experience, an eye-opening trip abroad, unexpected career transitions, and more.</p><ol><li><a href="https://www.sciencemag.org/careers/2019/10/why-scientists-should-take-more-coffee-breaks">Why scientists should take more coffee breaks</a><br>Vivienne Tam argued that it’s important for grad students to make time for casual conversations with peers.</li><li><a href="https://www.sciencemag.org/careers/2019/06/how-i-learned-teach-scientist">How I learned to teach like a scientist</a><br>Sally Hoskins reflected on how she challenged students to think beyond the facts during her career as a college professor.</li><li><a href="https://www.sciencemag.org/careers/2019/07/committee-members-shouldn-t-expect-phd-students-serve-coffee-and-pastries">Committee members shouldn’t expect Ph.D. students to serve coffee and pastries</a><br>Kate Bredbenner wrote that thesis defenses and committee meetings are stressful enough without the added expectation of bringing food.</li><li><a href="https://www.sciencemag.org/careers/2019/10/reviewers-don-t-be-rude-nonnative-english-speakers">Reviewers, don’t be rude to nonnative English speakers</a><br>Adriana Romero-Olivares offered three principles for providing constructive, respectful feedback during the peer-review process.</li><li><a href="https://www.sciencemag.org/careers/2019/01/my-first-postdoc-position-was-disaster-what-i-learned">My first postdoc position was a disaster. This is what I learned</a><br>Victor Wong wrote that he should have quit his first postdoc and moved on much sooner.</li><li><a href="https://www.sciencemag.org/careers/2019/04/academia-hard-work-expected-taking-break-effort-well-spent-too">In academia, hard work is expected—but taking a break is effort well spent, too</a><br>Mattias Björnmalm reflected on why it’s important to take time away from work.</li><li><a href="https://www.sciencemag.org/careers/2019/04/academia-hard-work-expected-taking-break-effort-well-spent-too">How I became easy prey to a predatory publisher</a><br>Alan Chambers recounted how an email and the pressure to publish led him astray.</li><li><a href="https://www.sciencemag.org/careers/2019/02/reimbursement-policies-make-academia-less-inclusive">Reimbursement policies make academia less inclusive</a><br>Jessica Sagers argued that having to pay conference expenses up front from personal accounts is a significant burden for early-career researchers.</li><li><a href="https://www.sciencemag.org/careers/2019/02/leaving-phd-takes-courage-and-it-doesn-t-mean-path-academic-success-over">Leaving a Ph.D. takes courage—and it doesn’t mean the path to academic success is over</a><br>Hendrik Huthoff wrote that leaving his first Ph.D. program was one of the most important professional decisions he ever made.</li><li><a href="https://www.sciencemag.org/careers/2019/12/how-i-let-go-my-guilt-mother-grad-school">How I let go of my guilt as a mother in grad school</a><br>Ashley Stenzel realized that her daughters have benefited from her pursuit of higher education.<h3 id="Honorable-mentions"><a href="#Honorable-mentions" class="headerlink" title="Honorable mentions:"></a>Honorable mentions:</h3><a href="https://www.sciencemag.org/careers/2019/08/how-i-conquered-my-fear-public-speaking-and-learned-give-effective-presentations">How I conquered my fear of public speaking and learned to give effective presentations</a><br><a href="https://www.sciencemag.org/careers/2018/07/its-never-too-late-stretch-your-wings-why-i-got-phd-age-66">It’s never too late to stretch your wings: Why I got a Ph.D. at age 66</a><br><a href="https://www.sciencemag.org/careers/2018/07/its-never-too-late-stretch-your-wings-why-i-got-phd-age-66">Three lessons from industry that I’m taking back to academia</a><br><a href="https://www.sciencemag.org/careers/2019/05/i-felt-lost-new-academic-culture-then-i-learned-about-hidden-curriculum">I felt lost in a new academic culture. Then I learned about the hidden curriculum</a><br><a href="https://www.sciencemag.org/careers/2019/10/post-phd-job-searches-are-tough-here-s-how-i-escaped-dr-seuss-s-waiting-place">Post-Ph.D. job searches are tough. Here’s how I escaped Dr. Seuss’s ‘Waiting Place’</a></li></ol>]]></content>
    
    
    <summary type="html">聚焦科学家在追求职业生涯中面临的个人和专业挑战的20篇文章</summary>
    
    
    
    <category term="P.h.D. career" scheme="https://yanjianzhang.github.io/categories/P-h-D-career/"/>
    
    
    <category term="P.h.D." scheme="https://yanjianzhang.github.io/tags/P-h-D/"/>
    
  </entry>
  
  <entry>
    <title>Commonsense Validation and Explanation</title>
    <link href="https://yanjianzhang.github.io/2019/12/20/Commonsence/"/>
    <id>https://yanjianzhang.github.io/2019/12/20/Commonsence/</id>
    <published>2019-12-20T16:00:00.000Z</published>
    <updated>2021-12-26T22:45:49.318Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要关注<a href="http://alt.qcri.org/semeval2020/index.php?id=tasks">SemEval-2020</a>常识验证与解释部分</p><h3 id="Task4-Commonsense-Validation-and-Explanation-常识验证和解释"><a href="#Task4-Commonsense-Validation-and-Explanation-常识验证和解释" class="headerlink" title="Task4: Commonsense Validation and Explanation (常识验证和解释)"></a>Task4: Commonsense Validation and Explanation (常识验证和解释)</h3><h4 id="Subtask1-Commonsense-Validation"><a href="#Subtask1-Commonsense-Validation" class="headerlink" title="Subtask1 Commonsense Validation"></a>Subtask1 Commonsense Validation</h4><p>给出两句有相似结构的陈述，任务就是检验哪句陈述相对符合常识，哪句不符合常识。E.g. (A: He put a turkey into the fridge) vs.(B: He put an elephant into the fridge). 正确答案：A相对符合，B相对不符合。<br>分析：这一个题目</p><h4 id="Subtask2-Commonsense-Explanation-Multi-Choice"><a href="#Subtask2-Commonsense-Explanation-Multi-Choice" class="headerlink" title="Subtask2 Commonsense Explanation (Multi-Choice)"></a>Subtask2 Commonsense Explanation (Multi-Choice)</h4><p>从subtask1承接而来。给出不符合常识陈述的标签，并给出三个候选原因，选择出最能够解释不符合常识那个陈述出错的原因。E.g.不符合常识的陈述：(He put an elephant into the fridge) 三个候选原因：(A: an elephant is much bigger than a fridge), (B: elephants are usually gray while fridges are usually white), and (C: an elephant cannot eat a fridge)。正确答案:A.</p><h4 id="Subtask3-Commonsense-Explanation-Generation"><a href="#Subtask3-Commonsense-Explanation-Generation" class="headerlink" title="Subtask3 Commonsense Explanation(Generation)"></a>Subtask3 Commonsense Explanation(Generation)</h4><p> 也从subtask1承接而来。给出不符合常识的陈述，并要求生成解释其不符合常识的原因。训练集、开发集和测试集中每个例子都会提供三条正确原因，以供训练、开发和测试。比赛中结果以BLEU值来衡量，最终结果会以人工评价的方法衡量。E.g.不符合常识的陈述：(He put an elephant into the fridge) 三条正确原因：(A: an elephant is much bigger than a fridge), (B: a fridge is much smaller than an elephant), and (C: an elephant cannot fit in a fridge)。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>与传统的NLI任务文本蕴含关系识别相比，文本蕴含识别中的contradiction, neutral, entailment是两个句子之间的矛盾，然而仅仅知道它们是矛盾的并不能得出他们的对错。<br>在这里的Commonsense Validation中，两个句子固然是矛盾的，这一点并没有办法用于选出哪个合理。<br>在Explanation中，陈述和回答里虽然确实有contradiction的意思，然而却需要额外的常识才能选出答案。</p><h4 id="与CommonsenseQA相比较"><a href="#与CommonsenseQA相比较" class="headerlink" title="与CommonsenseQA相比较"></a>与CommonsenseQA相比较</h4><p>CommonsenseQA还是存在相关的选项的文本经常与题干同时出现的问题的文本关联性的影响，可以说解决了CommonsenseQA的问题还不一定能够解决Commonsense Explanation的问题，而Commonsense Validation还是能够通过相似的方法解决的。</p>]]></content>
    
    
    <summary type="html">本文主要关注SemEval-2020常识验证与解释部分</summary>
    
    
    
    <category term="Natural Language Processing" scheme="https://yanjianzhang.github.io/categories/Natural-Language-Processing/"/>
    
    
    <category term="AI" scheme="https://yanjianzhang.github.io/tags/AI/"/>
    
    <category term="Commonsense" scheme="https://yanjianzhang.github.io/tags/Commonsense/"/>
    
  </entry>
  
  <entry>
    <title>Dropout of RNN, batch size, accumulation steps</title>
    <link href="https://yanjianzhang.github.io/2019/12/03/encoder-decoder/"/>
    <id>https://yanjianzhang.github.io/2019/12/03/encoder-decoder/</id>
    <published>2019-12-03T16:00:00.000Z</published>
    <updated>2021-12-26T22:48:52.876Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Srivastava et al. (2014) applied dropout to feed forward neural network’s and RBM’s and noted a probability of dropout around <strong>0.5 for hidden units and 0.2 for inputs</strong> worked well for a variety of tasks.</p><p>Reference: <a href="https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b">A review of Dropout as applied to RNNs</a></p><p>When I apply the 0.5 for hidden units and 0.2 for inputs, it works well. But it is not the case in decoder. <strong>In decoder, I would suggest not to use dropout</strong>.</p><h3 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h3><p>Suggest &lt;=32， See <a href="https://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf">Efficient Mini-batch Training for Stochastic Optimization</a> and <a href="https://svail.github.io/rnn_perf/">this RNN study</a></p><h3 id="Accumulation-steps"><a href="#Accumulation-steps" class="headerlink" title="Accumulation_steps"></a>Accumulation_steps</h3><p>Have the same function like batch size, but it can be use when Graphics memory is not enough.</p>]]></content>
    
    
    <summary type="html">The selection of dropout rate for encoder-decoder mechanism</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yanjianzhang.github.io/categories/Deep-Learning/"/>
    
    
    <category term="RNN" scheme="https://yanjianzhang.github.io/tags/RNN/"/>
    
    <category term="Dropout" scheme="https://yanjianzhang.github.io/tags/Dropout/"/>
    
  </entry>
  
  <entry>
    <title>Indent problem! Latex and Python</title>
    <link href="https://yanjianzhang.github.io/2019/12/03/indent/"/>
    <id>https://yanjianzhang.github.io/2019/12/03/indent/</id>
    <published>2019-12-03T16:00:00.000Z</published>
    <updated>2021-12-26T22:50:53.987Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>When we cooperation with other within python, like in <a href="https://studio.dev.tencent.com/">Cloud studio</a> of tencent. We may find our indent is different with our friend when we input the code. You may make a file name  “.editorconfig” in the root directory.</p><pre><code>root = true[*]charset = utf-8indent_style = tabtab_width = 4end_of_line = lfinsert_final_newline = truetrim_trailing_whitespace = true</code></pre><p>However, when the python code are already mixed with tabs and spaces. Don’t worry.<br>Select Pycharm -&gt; Edit -&gt; Convert Indents. You will transform the whole file quickly without manually do it.</p><h3 id="Matlab"><a href="#Matlab" class="headerlink" title="Matlab"></a>Matlab</h3><p>When you copy the code to texstudio(seems not happend in Overleaf), the indent will vanish. It is the problem of default setting of Texstudio.<br>Select Options -&gt; Configure Texstudio -&gt; Editor -&gt; Indentation mode. And choose Keep Indentation.<br>That will solve the problem.</p><p>&nbsp; &nbsp;<br>&nbsp; &nbsp;<br>&nbsp; &nbsp;</p><h5 id="Reference-Version-in-Chinese-from-Google-translation"><a href="#Reference-Version-in-Chinese-from-Google-translation" class="headerlink" title="Reference Version in Chinese from Google translation:"></a>Reference Version in Chinese from Google translation:</h5><h3 id="Python-1"><a href="#Python-1" class="headerlink" title="Python"></a>Python</h3><p>当我们在python中与其他人合作时，例如腾讯的[Cloud studio]（<a href="https://studio.dev.tencent.com/）。">https://studio.dev.tencent.com/）。</a> 输入代码时，我们可能会发现缩进与我们的朋友有所不同。 您可以在根目录创建一个文件名“ .editorconfig”。</p><pre><code>根=真[*]字符集= utf-8indent_style =制表符tab_width = 4end_of_line = lfinsert_final_newline = truetrim_trailing_whitespace = true</code></pre><p>但是，当python代码已经与制表符和空格混合在一起时。 不用担心<br>选择Pycharm-&gt;编辑-&gt;转换缩进。 您无需手动进行操作即可快速转换整个文件。</p><h3 id="Matlab-1"><a href="#Matlab-1" class="headerlink" title="Matlab"></a>Matlab</h3><p>当您将代码复制到texstudio时（似乎在Overleaf中没有发生），缩进将消失。 这是Texstudio默认设置的问题。<br>选择选项-&gt;配置Texstudio-&gt;编辑器-&gt;缩进模式。 然后选择“保持缩进”。<br>这样可以解决问题。</p>]]></content>
    
    
    <summary type="html">快速解决python的tab-space转换和latex复制代码时indent消失的问题</summary>
    
    
    
    <category term="Latex" scheme="https://yanjianzhang.github.io/categories/Latex/"/>
    
    
    <category term="Python" scheme="https://yanjianzhang.github.io/tags/Python/"/>
    
    <category term="Latex" scheme="https://yanjianzhang.github.io/tags/Latex/"/>
    
  </entry>
  
  <entry>
    <title>Matlab的使用笔记</title>
    <link href="https://yanjianzhang.github.io/2018/11/14/Matlab/"/>
    <id>https://yanjianzhang.github.io/2018/11/14/Matlab/</id>
    <published>2018-11-14T16:44:02.000Z</published>
    <updated>2018-11-15T16:59:48.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>显示变量的值<br>disp(x)</p><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><p>矩阵的生成方式,既可以用空格也可以用逗号来隔开元素<br>A = [1 2 3 4]<br>多行的向量的矩阵使用分号隔开<br>A = [1 2 3; 4 5 6; 7 8 10]<br>ones、zeros 或 rand 等函数生成列向量<br>z = zeros(5,1)</p><p>矩阵的转置使用单引号’<br>A’<br>标准矩阵乘法<em><br>A</em>inv(A)<br>元素级乘法<br>.<em><br>元素级运算<br>.^3,.</em>3,./3</p><p>Matlab优先处理矩阵的列，如下幻方矩阵<br>A = [16 3 2 13; 5 10 11 8; 9 6 7 12; 4 15 14 1]<br>sum(A),为列相加得到的行向量<br>结果为：ans =<br>    34    34    34    34</p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>横向串联<br>A = [A,A]<br>纵向串联<br>A = [A;A]<br>复数i = sqrt(-1),可以直接写作i</p><h3 id="文本和字符串"><a href="#文本和字符串" class="headerlink" title="文本和字符串"></a>文本和字符串</h3><p>文本使用单引号表示<br>myText = ‘Hello, world’;<br>字符串中的单引号需要使用两个单引号表示<br>otherText = ‘You’’re right’<br>方括号串联字符数组，就像串联数值数组一样<br>longText = [myText,’ - ‘,otherText]</p><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><p>如果存在多个输出参数，请将其括在方括号中</p><p>[maxA,location] = max(A)<br>clc 函数清除命令行窗口</p><h3 id="二维图和三维图"><a href="#二维图和三维图" class="headerlink" title="二维图和三维图"></a>二维图和三维图</h3><p><a href="https://ww2.mathworks.cn/help/matlab/learn_matlab/plots.html">链接</a></p><h3 id="构建表"><a href="#构建表" class="headerlink" title="构建表"></a>构建表</h3><p>使用列向量构建表<br>n = (0:9)’;<br>pows = [n  n.^2  2.^n]<br>结果为<br>pows =<br>     0     0     1<br>     1     1     2<br>     2     4     4<br>     3     9     8<br>     4    16    16<br>     5    25    32<br>     6    36    64<br>     7    49   128<br>     8    64   256<br>     9    81   512</p><h3 id="逻辑下标"><a href="#逻辑下标" class="headerlink" title="逻辑下标"></a>逻辑下标</h3><p>相当于找到数组中满足条件的部分，例如<br>x = [2.1 1.7 1.6 1.5 NaN 1.9 1.8 1.5 5.1 1.8 1.4 2.2 1.6 1.8];<br>x = x(isfinite(x))<br>结果为<br>x =<br>  2.1 1.7 1.6 1.5 1.9 1.8 1.5 5.1 1.8 1.4 2.2 1.6 1.8</p><h3 id="find函数"><a href="#find函数" class="headerlink" title="find函数"></a>find函数</h3><p>find 函数可用于确定与指定逻辑条件相符的数组元素的索引。<br>find 以最简单的形式返回索引的列向量。转置该向量以便获取索引的行向量。<br>如幻方矩阵<br>k = find(isprime(A))’<br>结果为<br>k =<br>     2     5     9    10    11    13<br>运行<br>A(k) = NaN<br>结果为<br>A =<br>    16   NaN   NaN   NaN<br>   NaN    10   NaN     8<br>     9     6   NaN    12<br>     4    15    14     1</p><h3 id="元胞数组"><a href="#元胞数组" class="headerlink" title="元胞数组"></a>元胞数组</h3><p>使用{}生成，使用三维数组可以存储相同大小的矩阵序列。元胞数组可用于存储不同大小的矩阵序列<br>*要操作包含不同长度的行的文本主体，您有两种选择，即使用填充的字符数组或使用字符向量元胞数组。创建字符数组时，数组各行的长度必须相同。（使用空格填充较短行的末尾。）char 函数可执行这种填充操作。例如，<br>S = char(‘A’,’rolling’,’stone’,’gathers’,’momentum.’)<br>生成一个 5×9 字符数组：<br>S =<br>A<br>rolling<br>stone<br>gathers<br>momentum.<br>再者，也可以将文本存储在元胞数组中。例如，<br>C = {‘A’;’rolling’;’stone’;’gathers’;’momentum.’}</p><h3 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h3><p>使用struct(“key”,”value”,”key”,”value”)生成<br>引用方法<br>S.score<br>S(2).name = ‘Toni Miller’;</p><h3 id="Matlab绘图"><a href="#Matlab绘图" class="headerlink" title="Matlab绘图"></a>Matlab绘图</h3><p><a href="https://ww2.mathworks.cn/help/matlab/learn_matlab/data-analysis.html">链接</a></p>]]></content>
    
    
    <summary type="html">Matlab使用，small tricky</summary>
    
    
    
    <category term="Matlab" scheme="https://yanjianzhang.github.io/categories/Matlab/"/>
    
    
    <category term="Matlab" scheme="https://yanjianzhang.github.io/tags/Matlab/"/>
    
  </entry>
  
  <entry>
    <title>机器学习评价指标</title>
    <link href="https://yanjianzhang.github.io/2018/11/08/measure/"/>
    <id>https://yanjianzhang.github.io/2018/11/08/measure/</id>
    <published>2018-11-08T16:44:02.000Z</published>
    <updated>2021-12-26T22:47:29.149Z</updated>
    
    <content type="html"><![CDATA[<h4 id="分类问题的评价指标"><a href="#分类问题的评价指标" class="headerlink" title="分类问题的评价指标"></a>分类问题的评价指标</h4><div class="table-container"><table><thead><tr><th>*</th><th>Relevant，正类</th><th>Nonrelevent，负类</th></tr></thead><tbody><tr><td>Retrieved 被检索到</td><td>TP</td><td>FP</td></tr><tr><td>Notretrieved未检索到</td><td>FN</td><td>TN</td></tr></tbody></table></div><p>准确率是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比</p><script type="math/tex; mode=display">A(accurate) = \frac{TP+TN}{TP+TN+FP+FN}</script><p>精确率是：它计算的是所有被检索到的item中,”应该被检索到”的item占的比例。</p><script type="math/tex; mode=display">P(percision) = \frac{TP}{TP+FP}</script><p>召回率是：它计算的是所有检索到的item占所有”应该检索到的item”的比例。</p><script type="math/tex; mode=display">R(recall) = \frac{TP}{TP+FN}</script><p>综合评价指标F-measure是精确度和召回率的综合</p><script type="math/tex; mode=display">F = \frac{(a^2+1)PR}{(a^2)P+R}</script><p>当a = 0时，为F1:</p><script type="math/tex; mode=display">F = \frac{PR}{P+R}</script><h4 id="机器翻译质量评测算法"><a href="#机器翻译质量评测算法" class="headerlink" title="机器翻译质量评测算法"></a>机器翻译质量评测算法</h4><p>使用BLEU来衡量算法的准确程度<br>计算公式如下</p><script type="math/tex; mode=display">BLEU = BP·exp(\sum_1^N w_nlog(p_n))</script><p>其中BP = $\begin{cases}1\space\space\space\space\space\space\space\space\space\space\space\space\space\space if \space c&gt;r \\e^{(1-r/c)}\space\space\space\space if \space c \le r\end{cases}<br>$<br>c = len(word of machine)<br>r = len(word of reference)<br>而 <script type="math/tex">p_n = \frac{\sum_{C\in Candidate}\sum_{n-gram\in C}Count_{clip}(n-gram)}{\sum_{C^{'}\in Candidate}\sum_{n-gram\in C'}Count_(n-gram)}</script></p>]]></content>
    
    
    <summary type="html">机器学习中的准确率、精确率、召回率以及F度量</summary>
    
    
    
    <category term="Machine Learing" scheme="https://yanjianzhang.github.io/categories/Machine-Learing/"/>
    
    
    <category term="Machine Learing" scheme="https://yanjianzhang.github.io/tags/Machine-Learing/"/>
    
  </entry>
  
  <entry>
    <title>Tensorboard的使用</title>
    <link href="https://yanjianzhang.github.io/2018/11/08/tensorboard/"/>
    <id>https://yanjianzhang.github.io/2018/11/08/tensorboard/</id>
    <published>2018-11-08T16:44:02.000Z</published>
    <updated>2021-12-26T22:47:01.471Z</updated>
    
    <content type="html"><![CDATA[<p>其中<br>summary.image生成新的图像用于验证输入结果的准确与否<br>summary.scalar用于记录准确率、损失等信息<br>得到的结果需要进行 merged = tf.summary.merge_all()<br>最后要sess.run([train_step,merged])</p><pre><code class="lang-python">import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamax_steps = 1000learning_rate = 0.001dropout = 0.9data_dir = &quot;&quot;log_dir = &quot;C:/Users/dongfanker/Desktop/log&quot;mnist = input_data.read_data_sets(data_dir, one_hot=True)sess = tf.InteractiveSession()with tf.name_scope(&quot;input&quot;):    x = tf.placeholder(tf.float32, [None, 784], name=&quot;x-input&quot;)    y_ = tf.placeholder(tf.float32, [None, 10], name=&quot;y-input&quot;)with tf.name_scope(&quot;input_reshape&quot;):    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])    tf.summary.image(&quot;input&quot;, image_shaped_input, 10)# 创建初始化参数的方法def weight_variable(shape):    initial = tf.truncated_normal(shape, stddev=0.1)    return tf.Variable(initial)def bias_variable(shape):    initial = tf.constant(0.1, shape=shape)    return tf.Variable(initial)def variable_summaries(var):    with tf.name_scope(&quot;summaries&quot;):        mean = tf.reduce_mean(var)        tf.summary.scalar(&quot;mean&quot;, mean)        with tf.name_scope(&quot;stddev&quot;):            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))        tf.summary.scalar(&quot;stddev&quot;, stddev)    tf.summary.scalar(&quot;stddev&quot;, stddev)    tf.summary.scalar(&quot;max&quot;, tf.reduce_max(var))    tf.summary.scalar(&quot;min&quot;, tf.reduce_min(var))    tf.summary.histogram(&quot;histogram&quot;, var)def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):    with tf.name_scope(layer_name):        with tf.name_scope(&quot;weights&quot;):            weights = weight_variable([input_dim, output_dim])            variable_summaries(weights)        with tf.name_scope(&quot;biases&quot;):            biases = bias_variable([output_dim])            variable_summaries(biases)        with tf.name_scope(&quot;linear_compute&quot;):            preactivate = tf.matmul(input_tensor, weights) + biases            tf.summary.histogram(&quot;pre_activation&quot;, preactivate)        activations = act(preactivate, name=&quot;activation&quot;)        tf.summary.histogram(&quot;activation&quot;, activations)        return activationshidden1 = nn_layer(x, 784, 500, &quot;layer1&quot;)with tf.name_scope(&quot;dropout&quot;):    keep_prob = tf.placeholder(tf.float32)    tf.summary.scalar(&quot;dropout_keep_probability&quot;, keep_prob)    dropped = tf.nn.dropout(hidden1, keep_prob)y = nn_layer(dropped, 500, 10, &quot;layer2&quot;, act=tf.identity)with tf.name_scope(&quot;cross_entropy&quot;):    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)    with tf.name_scope(&quot;total&quot;):        cross_entropy = tf.reduce_mean(diff)tf.summary.scalar(&quot;loss&quot;, cross_entropy)with tf.name_scope(&quot;train&quot;):    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)with tf.name_scope(&quot;accuracy&quot;):    with tf.name_scope(&quot;correct_prediction&quot;):        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))    with tf.name_scope(&quot;accuracy&quot;):        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))tf.summary.scalar(&quot;accuracy&quot;, accuracy)merged = tf.summary.merge_all()train_writer = tf.summary.FileWriter(log_dir + &quot;/train&quot;, sess.graph)test_writer = tf.summary.FileWriter(log_dir + &quot;/test&quot;)tf.global_variables_initializer().run()def feed_dict(train):    if train:        xs, ys = mnist.train.next_batch(100)        k = dropout    else:        xs, ys = mnist.test.images, mnist.test.labels        k = 1.0    return &#123;x: xs, y_: ys, keep_prob: k&#125;saver = tf.train.Saver()for i in range(max_steps):    if i % 10 == 0:        summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))        test_writer.add_summary(summary, i)        print(&quot;Accuracy at step %s: %s&quot; % (i, acc))    else:        if i % 100 == 99:            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)            run_metadata = tf.RunMetadata()            summary, _ = sess.run([merged, train_step],                                  feed_dict=feed_dict(True),                                  options=run_options,                                  run_metadata=run_metadata)            train_writer.add_run_metadata(run_metadata, &quot;step%03d&quot; % i)            train_writer.add_summary(summary, i)            saver.save(sess, log_dir + &quot;/model.ckpt&quot;, i)            print(&quot;Adding run metadata for &quot;, i)        else:            summary, _ = sess.run([merged, train_step],                                  feed_dict=feed_dict(True))            train_writer.add_summary(summary, i)train_writer.close()test_writer.close()</code></pre>]]></content>
    
    
    <summary type="html">学习Tensorboard的使用</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yanjianzhang.github.io/categories/Deep-Learning/"/>
    
    
    <category term="tensorflow" scheme="https://yanjianzhang.github.io/tags/tensorflow/"/>
    
    <category term="tensorboard" scheme="https://yanjianzhang.github.io/tags/tensorboard/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow 基本概念</title>
    <link href="https://yanjianzhang.github.io/2018/11/07/tfbase/"/>
    <id>https://yanjianzhang.github.io/2018/11/07/tfbase/</id>
    <published>2018-11-07T16:44:02.000Z</published>
    <updated>2021-12-26T22:47:16.172Z</updated>
    
    <content type="html"><![CDATA[<h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>Tensorflow程序的计算可分为两个阶段，在第一个阶段定义所有计算图中的计算，第二阶段为执行计算。<br>下面定义一个计算，生成计算图</p><pre><code class="lang-python">import tensorflow as tfa = tf.constant([1.0, 2.0], name=&quot;a&quot;)b = tf.constant([2.0, 3.0], name=&quot;b&quot;)result = a + b print(tf.get_default_graph())print(a.graph)print(b.graph)</code></pre><p>result: 均属于同一个计算图</p><pre><code>&lt;tensorflow.python.framework.ops.Graph object at 0x0000012C137DAD68&gt;&lt;tensorflow.python.framework.ops.Graph object at 0x0000012C137DAD68&gt;&lt;tensorflow.python.framework.ops.Graph object at 0x0000012C137DAD68&gt;</code></pre><p>还可以通过tf.Graph函数来生成新的计算图，代码如下，注意注释的部分已经被淘汰</p><pre><code class="lang-python">import tensorflow as tfg1 = tf.Graph()with g1.as_default():    v1 = tf.get_variable(        # &quot;v&quot;, initializer=tf.zeros_initializer(shape[1])        &quot;v1&quot;, shape=[4], initializer=tf.zeros_initializer()    )    v2 = tf.get_variable(        &quot;v2&quot;, shape=[4], initializer=tf.ones_initializer()    )with tf.Session(graph=g1) as sess:    # # tf.initialize_all_variables().run()    sess.run(tf.global_variables_initializer())    with tf.variable_scope(&quot;&quot;, reuse=True):        print(sess.run(tf.get_variable(&quot;v1&quot;)))        print(sess.run(tf.get_variable(&quot;v2&quot;)))</code></pre><p>result</p><pre><code>[ 0.  0.  0.  0.][ 1.  1.  1.  1.]</code></pre><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p>下面介绍张量</p><pre><code class="lang-python">import tensorflow as tfa = tf.constant([1, 2], name=&quot;a&quot;, dtype=tf.float32)b = tf.constant([2.0, 3.0], name=&quot;b&quot;)result1 = a + b# 此时还没有结果print(result1)result2 = tf.add(a, b, name=&quot;add&quot;)# 此时才会有结果print(result2)</code></pre><p>result：可以看见在result1中默认会命名“+”为add，后面相同的名字TensorFlow会进行修改</p><pre><code>Tensor(&quot;add:0&quot;, shape=(2,), dtype=float32)Tensor(&quot;add_1:0&quot;, shape=(2,), dtype=float32)</code></pre><h4 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h4><p>下面是会话<br>创建会话可以通过两种模式创建，一种是明确创建和关闭会话的函数</p><pre><code class="lang-python">sess = tf.Session()sess.run(...)sess.close()</code></pre><p>另一种就直接通过上下文管理器来管理会话，使用with</p><pre><code class="lang-python">with tf.Session() as sess:    sess.run(...)</code></pre><p>配置GPU的使用,其中allow_soft_placement会使得某些不被GPU支持的运算放入CPU中，而log_device_placement选择是否打印日记，可以记录那个节点被安排到哪个设备方便调试，而在生产环境中设为False,可以减少日志量</p><pre><code>config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)with tf.Session(graph=g1, config=config) as sess:    ......</code></pre><h4 id="前向传播示例"><a href="#前向传播示例" class="headerlink" title="前向传播示例"></a>前向传播示例</h4><p>下面是一个简单的神经网络的前向传播的过程,注意x为列向量，有两个[]</p><pre><code class="lang-python">import tensorflow as tfw1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))print(w1, w2)x = tf.constant([[0.7, 0.9]])a = tf.matmul(x, w1)y = tf.matmul(a, w2) #此处是product# 也可以是 with tf.Session as sess:sess = tf.Session()# 此处需要进行初始化sess.run(tf.global_variables_initializer()) # 获得所想要得到的运算结果print(sess.run(y)) #对product进行运算sess.close()</code></pre><p>result</p><pre><code>[[ 3.95757794]]</code></pre><h4 id="placeholder"><a href="#placeholder" class="headerlink" title="placeholder"></a>placeholder</h4><pre><code class="lang-python">input1 = tf.placeholder(tf.float32)</code></pre><p>之后需要feed才能run</p><h4 id="添加层"><a href="#添加层" class="headerlink" title="添加层"></a>添加层</h4><pre><code class="lang-python">def add_layer(inputs,in_size,out_size,activation_fun = None):    weight = tf.VAriable(tf.random_normal([in_size,out_size]))    biase = ...    if activation != None:        return activation(input*weight+biase)</code></pre><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><pre><code class="lang-python"># None 表示任意数量xs = tf.placeholder(tf.float32,[None, 784]) # 28*28ys = tf.placeholder(tf.float32,[None, 10]) # 10</code></pre><h4 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h4><pre><code>l1 = add_layer(xs, 1, 10 activation = tf.nn.relu)loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices = [1]))train_step = tf.train.Gradient...sess.run(...)</code></pre><h4 id="GPU-support"><a href="#GPU-support" class="headerlink" title="GPU support"></a>GPU support</h4><pre><code class="lang-python">config = tf.ConfigProto()config.gpu_options.allow_growth = True # 按需分配，不全部占用for d in [&quot;/device:GPU:2&quot;,&quot;/device:GPU:3&quot;] #使用多块GPU    with tf.device(d):</code></pre>]]></content>
    
    
    <summary type="html">温习一下TensorFlow的基本概念</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yanjianzhang.github.io/categories/Deep-Learning/"/>
    
    
    <category term="tensorflow" scheme="https://yanjianzhang.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>字符串和文本处理</title>
    <link href="https://yanjianzhang.github.io/2018/11/06/cookbookString/"/>
    <id>https://yanjianzhang.github.io/2018/11/06/cookbookString/</id>
    <published>2018-11-06T16:44:02.000Z</published>
    <updated>2021-12-26T22:46:42.727Z</updated>
    
    <content type="html"><![CDATA[<h4 id="字符串的切分"><a href="#字符串的切分" class="headerlink" title="字符串的切分"></a>字符串的切分</h4><pre><code class="lang-python">line = &quot;爸爸 妈妈; 儿子, 女儿,弟弟, 妹妹&quot;import reprint(re.split(r&#39;[;,\s]\s*&#39;, line))print(re.split(r&#39;(;|,|\s)\s*&#39;, line))fields = re.split(r&#39;(;|,|\s)\s*&#39;, line)print(fields[::2])print(fields[1::2])</code></pre><p>result:</p><pre><code>[&#39;爸爸&#39;, &#39;妈妈&#39;, &#39;儿子&#39;, &#39;女儿&#39;, &#39;弟弟&#39;, &#39;妹妹&#39;][&#39;爸爸&#39;, &#39; &#39;, &#39;妈妈&#39;, &#39;;&#39;, &#39;儿子&#39;, &#39;,&#39;, &#39;女儿&#39;, &#39;,&#39;, &#39;弟弟&#39;, &#39;,&#39;, &#39;妹妹&#39;][&#39;爸爸&#39;, &#39;妈妈&#39;, &#39;儿子&#39;, &#39;女儿&#39;, &#39;弟弟&#39;, &#39;妹妹&#39;][&#39; &#39;, &#39;;&#39;, &#39;,&#39;, &#39;,&#39;, &#39;,&#39;]</code></pre><p>总结: “\s” 代表空格，一定范围的符号用[]、(…|…|…)框起来，使用[]会去掉匹配的文本，而(…|…|…)会保留匹配的分组，可变长(0-n)的字段可以加*l，在末尾的field的切片中，最后一位代表step</p><h4 id="字符串的开头与结尾匹配"><a href="#字符串的开头与结尾匹配" class="headerlink" title="字符串的开头与结尾匹配"></a>字符串的开头与结尾匹配</h4><pre><code class="lang-python">filenames = [&quot;Makefile&quot;, &quot;foo.c&quot;, &quot;bar.py&quot;, &quot;spam.c&quot;, &quot;spam.h&quot;]print(any(name.endswith(&quot;.py&quot;) for name in filenames))print([name for name in filenames if name.endswith((&quot;.c&quot;, &quot;.h&quot;))])</code></pre><p>result</p><pre><code>True[&#39;foo.c&#39;, &#39;spam.c&#39;, &#39;spam.h&#39;]</code></pre><p>总结：直接使用str.startswith()，str.endswith()，any用于检验列表元素的有无，两个函数可以作为判断指标生成新的列表，注意这两个函数内部如果有多个匹配的话需要用tuple(元组)把它们括起来<br>也可以使用match来进行开头的匹配，代码如下</p><pre><code class="lang-python">import reurl = &quot;http://www.python.org&quot;res = re.match(&quot;https|http|ftp&quot;, url)print(res != None)print(res.group())</code></pre><p>result</p><pre><code>Truehttp</code></pre><h4 id="字符串的匹配与搜索"><a href="#字符串的匹配与搜索" class="headerlink" title="字符串的匹配与搜索"></a>字符串的匹配与搜索</h4><pre><code class="lang-python">import retext = &quot;Today is 11/27/2012. PyCon starts 3/13/2013&quot;print(text.find(&quot;PyCon&quot;))print(re.findall(r&#39;\d+/\d+/\d+&#39;, text))datapat = re.compile(r&#39;\d+/\d+/\d+&#39;)print(datapat.findall(text))</code></pre><p>result</p><pre><code>21[&#39;11/27/2012&#39;, &#39;3/13/2013&#39;][&#39;11/27/2012&#39;, &#39;3/13/2013&#39;]</code></pre><p>find可以返回第一个匹配的位置，如果想要找到所有的匹配，可以使用findall，一方面可以直接使用，另一方面可以将模式字符串预编译为模式对象</p>]]></content>
    
    
    <summary type="html">python的字符串与文本处理的技巧</summary>
    
    
    
    <category term="Natural Language Processing" scheme="https://yanjianzhang.github.io/categories/Natural-Language-Processing/"/>
    
    
    <category term="string process" scheme="https://yanjianzhang.github.io/tags/string-process/"/>
    
  </entry>
  
  <entry>
    <title>Trie树</title>
    <link href="https://yanjianzhang.github.io/2018/11/03/trie/"/>
    <id>https://yanjianzhang.github.io/2018/11/03/trie/</id>
    <published>2018-11-03T16:44:02.000Z</published>
    <updated>2021-12-26T22:46:11.461Z</updated>
    
    <content type="html"><![CDATA[<p>为了提高我的单词查找的速度，我拜读了<a href="https://github.com/Rshcaroline/FDU-Natural-Language-Processing/blob/master/Project%201.%20Spell%20Correction/spell_correction.py">@Rshcaroline</a>的代码，发现trie树是一个很好解决这个问题的方法</p><p>Trie树，又叫字典树、前缀树（Prefix Tree）、单词查找树 或 键树，是一种多叉树结构</p><p>Trie树的基本性质：</p><ol><li>根节点不包含字符，除根节点外的每一个子节点都包含一个字符。</li><li>从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。</li><li>每个节点的所有子节点包含的字符互不相同。<br>通常在实现的时候，会在节点结构中设置一个标志，用来标记该结点处是否构成一个单词（关键字）。</li></ol><p>为了了解它的实现，我从leetcode中找了题目<a href="https://leetcode.com/problems/implement-trie-prefix-tree/description/">Implement Trie (Prefix Tree)</a>做了一下</p><p>一下是我的代码</p><pre><code class="lang-python">class Trie:    def __init__(self):        &quot;&quot;&quot;        Initialize your data structure here.        &quot;&quot;&quot;        self.root = &#123;&#125;        self.END = &quot;$&quot;    def insert(self, word):        &quot;&quot;&quot;        Inserts a word into the trie.        :type word: str        :rtype: void        &quot;&quot;&quot;        t = self.root        for c in word:            if c not in t:                t[c] = &#123;&#125;            t = t[c]                        t[self.END] = &#123;&#125;            def search(self, word):        &quot;&quot;&quot;1        Returns if the word is in the trie.        :type word: str        :rtype: bool        &quot;&quot;&quot;        t = self.root        for c in word:            if c not in t:                return False            else:                t = t[c]            if self.END not in t:            return False        return True    def startsWith(self, prefix):        &quot;&quot;&quot;        Returns if there is any word in the trie that starts with the given prefix.        :type prefix: str        :rtype: bool        &quot;&quot;&quot;        t = self.root        for c in prefix:            if c not in t:                return False            else:                t = t[c]        return True</code></pre><p>对于一个单词的查找一定编辑距离的相关集合的代码如下(from <a href="https://github.com/Rshcaroline/FDU-Natural-Language-Processing/blob/master/Project%201.%20Spell%20Correction/spell_correction.py">@Rshcaroline</a>)，其中的deque是python的高性能双向队列，用于将trie,  word, path, edit_distance整个打包起来加入队列, 广度优先查找符合的单词</p><pre><code class="lang-python">def get_candidate(trie, word, edit_distance=1):    que = deque([(trie, word, &#39;&#39;, edit_distance)])    while que:        trie, word, path, edit_distance = que.popleft()        if word == &#39;&#39;:            if END in trie:                yield path            # 词尾增加字母            if edit_distance &gt; 0:                for k in trie:                    if k != END:                        que.appendleft((trie[k], &#39;&#39;, path+k, edit_distance-1))        else:            if word[0] in trie:                # 首字母匹配成功                que.appendleft((trie[word[0]], word[1:], path+word[0], edit_distance))            # 无论首字母是否匹配成功，都如下处理            if edit_distance &gt; 0:                edit_distance -= 1                for k in trie.keys() - &#123;word[0], END&#125;:                    # 用k替换余词首字母，进入trie[k]                    que.append((trie[k], word[1:], path+k, edit_distance))                    # 用k作为增加的首字母，进入trie[k]                    que.append((trie[k], word, path+k, edit_distance))                # 删除目标词首字母，保持所处结点位置trie                que.append((trie, word[1:], path, edit_distance))                # 交换目标词前两个字母，保持所处结点位置trie                if len(word) &gt; 1:                    que.append((trie, word[1]+word[0]+word[2:], path, edit_distance))</code></pre>]]></content>
    
    
    <summary type="html">学习trie树,加快单词查找效率</summary>
    
    
    
    <category term="Data Structure" scheme="https://yanjianzhang.github.io/categories/Data-Structure/"/>
    
    
    <category term="Trie" scheme="https://yanjianzhang.github.io/tags/Trie/"/>
    
  </entry>
  
  <entry>
    <title>Good-Turing、Absolute、kneser-ney smooth</title>
    <link href="https://yanjianzhang.github.io/2018/11/03/kneser-ney/"/>
    <id>https://yanjianzhang.github.io/2018/11/03/kneser-ney/</id>
    <published>2018-11-03T16:44:02.000Z</published>
    <updated>2021-12-26T23:13:47.090Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Good-Turing-smoothing"><a href="#Good-Turing-smoothing" class="headerlink" title="Good-Turing smoothing"></a>Good-Turing smoothing</h3><p>Good-Turing基本思想是：用观察计数较高的N元语法数重新估计概率量的大小，并把它指派给那些具有零计数或者较低计数的N元语法。<br>公式:</p><script type="math/tex; mode=display">c^{×} = \frac{(c+1)N_{c+1}}{N_c}</script><p>其中c为某个N-gram出现的频数, $N_c$ 为出现次数为c的N-Gram的词组的个数，$c^×$为Good-Turing平滑计数<br>例子:对于a = [A,A,A,B,B,C,D,E] sum(len(a)) = 7<br>$c_A = 3$    $c_C = 2$</p><p>$c_B = c_D = c_E = 1$</p><p>$N_1=4$ $N_2=1$ $N_3=1$ </p><p>thus </p><script type="math/tex; mode=display">c^{×} = 4 × \frac{0}{3} = 0</script><script type="math/tex; mode=display">c^×_B = 3×\frac{1}{1} = 3</script><script type="math/tex; mode=display">c^×_C = c^×_D = c^×_E = 2×\frac{1}{4} = \frac{1}{2}</script><p>（注意在他们求概率的时候都需要除以N = 7）<br>然而这样会导致最高频率的结果最后的计数为0，解决方法如下</p><ol><li>可以对于较低的计数使用$P_{GT}$,而对于较高的计数直接使用$P_{MLE} = \frac{c}{N}$</li><li>也可以在较大的计数的时候使用$F(r) = ar^b $,其中a，b为参数，b&lt;-1</li></ol><p>最终会出现概率之和不为0的情况，这时候要进行归一化，固定没有见过的结果的概率，将已经见过的概率之和归一化使得共同的概率结果为1<br>归一化例子可见于<a href="https://www.csd.uwo.ca/courses/CS4442b/L9-NLP-LangModels.pdf">L9-NLP-LangModels.pdf</a>第64页</p><h3 id="Absolute-discounting"><a href="#Absolute-discounting" class="headerlink" title="Absolute discounting"></a>Absolute discounting</h3><p>一般的interpolation是利用高阶的模型的$P_MLE$乘以一个$\lambda$,而此处是从每个非零计数中减掉一个固定的$\delta \in (0,1)$,一般取$\delta = 0.75$<br>在bigram中，公式为</p><script type="math/tex; mode=display">P_{AD}(w_i|w_{i-1}) = \frac{c(w_{i-1},w_i)-\delta}{c(w_{i-1})}+\lambda(w_{i-1})P(w))</script><p>其中$\lambda(w_{i-1})为插值权重系数$<br>于是完整的公式就是</p><script type="math/tex; mode=display">p_{abs}(w_i|w^i_{i-n+1}) = \frac{max\{c(w^i_{i-n+1})-\delta,0\}}{\sum_{w_i}c(w^i_{i-n+1})}+(1-\lambda_{w^{i-1}_{i-n+1}}p_{ab}(w_i|w^{i-1}{i-n+2}))</script><p>为了使得结果的总和为1</p><script type="math/tex; mode=display">1 - \lambda _ { w _ { i - n + 1 } } ^ { i - 1 } = \frac { \delta } { \sum _ { w _ { i } } c ( w _ { i - n + 1 } ^ { i } ) } N _ { 1 + } ( w _ { i - n + 1 } ^ { i - 1 }  \mathbf { \bullet } )</script><p>其中</p><script type="math/tex; mode=display">N _ { 1 + } \left( w _ { i - n + 1 } ^ { i - 1 }  \bullet \right) = \left| \left\{ w _ { i } : c \left( w _ { i - n + 1 } ^ { i - 1 } w _ { i } \right) > 0 \right\} \right|</script><h3 id="Kneser-Ney-smoothing"><a href="#Kneser-Ney-smoothing" class="headerlink" title="Kneser-Ney smoothing"></a>Kneser-Ney smoothing</h3><p>bigram下的公式:</p><script type="math/tex; mode=display">p _ { K N } \left( w _ { i } | w _ { i - 1 } \right) = \frac { \max \left( c \left( w _ { i - 1 } , w _ { i } \right) - \delta , 0 \right) } { \sum _ { w ^ { \prime } } c \left( w _ { i - 1 } , w ^ { \prime } \right) } + \lambda _ { w _ { i - 1 } } p _ { K N } \left( w _ { i } \right)</script><p>其中</p><script type="math/tex; mode=display">p _ { K N } \left( w _ { i } \right) = \frac { \left| \left\{ w ^ { \prime } : 0 < c \left( w ^ { \prime } , w _ { i } \right) \right\} \right| } { \left| \left\{ \left( w ^ { \prime } , w ^ { \prime \prime } \right) : 0 < c \left( w ^ { \prime } , w ^ { \prime \prime } \right) \right\} \right| }</script><p>为的是求解在一个不熟悉的上下文中看见单词$w_i$的可能性,这使用$w_i$在出现在所有单词的次数和除以所有bigram的和来衡量<br>减掉一个固定的$\delta \in (0,1)$,一般取$\delta = 0.75$<br>$\lambda_{w_{i-1}}$是用来平衡使得条件概率$p _ { K N } ( w _ { i } | w _ { i - 1 } )$的总和为1的系数<br>得出满足条件的$\lambda_{w_{i-1}}$结果为</p><script type="math/tex; mode=display">\lambda _ { w _ { i - 1 } } = \frac { \delta } { \sum _ { w ^ { \prime } } c \left( w _ { i - 1 } , w ^ { \prime } \right) } \left| \left\{ w ^ { \prime } : 0 < c \left( w _ { i - 1 } , w ^ { \prime } \right) \right\} \right|</script><p>可以推广到n-gram</p><script type="math/tex; mode=display">p _ { K N } \left( w _ { i } | w _ { i - n + 1 } ^ { i - 1 } \right) = \frac { \max \left( c \left( w _ { i - n + 1 } ^ { i - 1 } , w _ { i } \right) - \delta , 0 \right) } { \sum _ { w ^ { \prime } } c \left( w _ { i - n + 1 } ^ { i - 1 } , w ^ { \prime } \right) } + \delta \frac { \left| \left\{ w ^ { \prime } : 0 < c \left( w _ { i - n + 1 } ^ { i - 1 } , w ^ { \prime } \right) \right\} \right| } { \sum _ { w _ { i } } c \left( w _ { i - n + 1 } ^ { i } \right) } p _ { K N } \left( w _ { i } | w _ { i - n + 2 } ^ { i - 1 } \right)</script>]]></content>
    
    
    <summary type="html">NLP中比较advance的smooth方法</summary>
    
    
    
    <category term="Natural Language Processing" scheme="https://yanjianzhang.github.io/categories/Natural-Language-Processing/"/>
    
    
    <category term="smooth" scheme="https://yanjianzhang.github.io/tags/smooth/"/>
    
    <category term="kneser-ney" scheme="https://yanjianzhang.github.io/tags/kneser-ney/"/>
    
  </entry>
  
  <entry>
    <title>Spring, Spring Boot, Restful, Docker</title>
    <link href="https://yanjianzhang.github.io/2018/09/16/spring/"/>
    <id>https://yanjianzhang.github.io/2018/09/16/spring/</id>
    <published>2018-09-15T22:00:00.000Z</published>
    <updated>2022-09-16T09:22:48.677Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、Restful"><a href="#一、Restful" class="headerlink" title="一、Restful"></a>一、Restful</h3><p>一个程序规范，一般会与http联系在一起。能够增加url的复用性，降低url的复杂性。<br>与GET、POST、PUT、DELETE等操作配合实现交互。</p><h3 id="二、Spring"><a href="#二、Spring" class="headerlink" title="二、Spring"></a>二、Spring</h3><p>一个JAVA框架, 通过高内聚的代码细节的隐藏，使得用户代码更加松耦合。</p><h4 id="IOC"><a href="#IOC" class="headerlink" title="IOC"></a>IOC</h4><p>Spring的重要逻辑之一，<strong>反转控制</strong>，将对象实例化的控制转移到外部代码中实现。常见的实现为DI(依赖注入)，外部代码将依赖对象产生后注入到内部代码里。<br>可以通过注解@autowired实现。</p><h4 id="Bean"><a href="#Bean" class="headerlink" title="Bean"></a>Bean</h4><p>Spring的重要概念之一，是<strong>能够暴露给Spring框架的特殊的类</strong>。可以通过注解@Controller、@Service、@repository产生，反转控制都是由Bean来完成的</p><h4 id="AOP"><a href="#AOP" class="headerlink" title="AOP"></a>AOP</h4><p>Spring的重要程序设计思想，<strong>面向切片编程</strong>，就是将一系列通用逻辑提取出来，进行复用。</p><h3 id="三、Spring-Boot"><a href="#三、Spring-Boot" class="headerlink" title="三、Spring Boot"></a>三、Spring Boot</h3><p>将Tomcat，Jetty等容器封装起来，使用application.yml文件控制，免去了大量使用中的初始化操作，使得Spring更易于使用。</p><h3 id="四、Docker"><a href="#四、Docker" class="headerlink" title="四、Docker"></a>四、Docker</h3><p>与虚拟机相比，虚拟机是将虚拟机系统与宿主操作系统分离，使用虚拟监视器来分隔，而Docker共用宿主的操作系统，因而不支持跨平台，但提供相对隔离的环境，同时有更高的效率。</p>]]></content>
    
    
    <summary type="html">Spring, Spring Boot, Restful，Docker的要点梳理</summary>
    
    
    
    <category term="Spring" scheme="https://yanjianzhang.github.io/categories/Spring/"/>
    
    
    <category term="Spring" scheme="https://yanjianzhang.github.io/tags/Spring/"/>
    
    <category term="Spring Boot" scheme="https://yanjianzhang.github.io/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>Vue项目bus与vuex的使用</title>
    <link href="https://yanjianzhang.github.io/2018/09/02/vue-bus/"/>
    <id>https://yanjianzhang.github.io/2018/09/02/vue-bus/</id>
    <published>2018-09-02T16:44:02.000Z</published>
    <updated>2018-09-02T17:39:46.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为什么需要Bus"><a href="#为什么需要Bus" class="headerlink" title="为什么需要Bus"></a>为什么需要Bus</h3><p>一般来说，都是利用父组件给子组件使用query或者params传递参数来实现子组件的数据显示<br>不过，当出现子组件需要向父组件传递数据的时候，就需要用到bus，bus可以自己创建，也可以通过装包来实现</p><h3 id="直接创建Bus"><a href="#直接创建Bus" class="headerlink" title="直接创建Bus"></a>直接创建Bus</h3><p>在此处直接将Bus注入Vue对象中，成为全局的组件。<br>在子组件中通过this.$root.Bus.$on(key,method),this.$root.Bus.$emit(key,data)来调用<br>将$on放在mounted，created这些钩子函数中，相应的函数使用(e)=&gt;{function}比较便捷         </p><pre><code class="lang-js">import Vue from &#39;vue&#39;const Bus = new Vue()var app= new Vue(&#123;    el:&#39;#app&#39;,　　 data:&#123;　　　　Bus    &#125;　　&#125;)</code></pre><h3 id="使用vue-bus"><a href="#使用vue-bus" class="headerlink" title="使用vue-bus"></a>使用vue-bus</h3><p>使用yarn或者npm安装vue-bus之后，在main.js中引用它</p><pre><code class="lang-js">import VueBus from &#39;vue-bus&#39;;Vue.use(VueBus);</code></pre><p>于是调用直接可以写为 this.$bus.on(key, this.method)，this.$bus.emit(key, { text: …… }<br>其中第一个字符串参数代表key，每个key都能够实现自己的独立传输，this.method为事先定义好的method，用于对传入的数据进行处理</p><h3 id="为什么使用vuex"><a href="#为什么使用vuex" class="headerlink" title="为什么使用vuex"></a>为什么使用vuex</h3><p>当我们的应用遇到多个组件共享状态时，会需要：</p><ol><li>多个组件依赖于同一状态。</li><li>来自不同组件的行为需要变更同一状态。</li></ol><p>经过我的观察，vuex在其中的作用就是组件与状态的捆绑剥离开来，使得组件状态的改变依赖于某个行为，这使得代码变得易于调试。<br>Vuex采用集中式存储管理应用的所有组件的状态，这里的关键在于集中式存储管理。这意味着本来需要共享状态的更新是需要组件之间通讯的，而现在有了vuex，就组件就都和store通讯了。</p><h3 id="使用vuex"><a href="#使用vuex" class="headerlink" title="使用vuex"></a>使用vuex</h3><p>使用yarn或者npm安装vuex之后，在main.js中引用它</p><pre><code class="lang-js">import Vuex from &#39;vuex&#39;import store from &#39;./vuex/store&#39;Vue.use(Vuex)new Vue(&#123;  el: &#39;#app&#39;,  store&#125;)</code></pre><p>随后创建vuex目录,将store.js放在目录下,定义state和mutation</p><pre><code class="lang-js">import Vue from &#39;vue&#39;import Vuex from &#39;vuex&#39;Vue.use(Vuex)const store = new Vuex.Store(&#123;  state: &#123;    author: &#39;Wise Wrong&#39;  &#125;,  mutations:&#123;      newAuthor(state,msg)&#123;          state.author = msg      &#125;  &#125;&#125;)export default store</code></pre><p>在使用的时候，不直接修改this.$store.state.author，而是使用this.$store.commit()来提交，可以让我们更好的跟踪每一个状态的变化，在大型项目中更为适用</p>]]></content>
    
    
    <summary type="html">Vue项目的全局传参与全项目状态管理</summary>
    
    
    
    <category term="Vue" scheme="https://yanjianzhang.github.io/categories/Vue/"/>
    
    
    <category term="Vue" scheme="https://yanjianzhang.github.io/tags/Vue/"/>
    
    <category term="router" scheme="https://yanjianzhang.github.io/tags/router/"/>
    
  </entry>
  
  <entry>
    <title>Vue项目路由传参</title>
    <link href="https://yanjianzhang.github.io/2018/09/01/Vue-route-pamas/"/>
    <id>https://yanjianzhang.github.io/2018/09/01/Vue-route-pamas/</id>
    <published>2018-09-01T07:44:02.000Z</published>
    <updated>2018-09-01T11:22:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="query传参"><a href="#query传参" class="headerlink" title="query传参"></a>query传参</h3><p>父组件通过path来配置路由，用query来传递参数</p><pre><code class="lang-js">    this.$router.push(&#123;          path: &#39;/describe&#39;,          query: &#123;            id: id          &#125;        &#125;)</code></pre><p>此时不需要在路由配置上做手脚，直接在子组件上使用params即可，注意此时是用route来表示，而不是用router<br>最终效果是query传递的参数会显示在url后面的?id=上</p><pre><code class="lang-js">this.$route.params.id</code></pre><h3 id="params传参"><a href="#params传参" class="headerlink" title="params传参"></a>params传参</h3><p>此时不过是用别名name来代替path，使用params代替query而已</p><pre><code class="lang-js">       this.$router.push(&#123;          name: &#39;Describe&#39;,          params: &#123;            id: id          &#125;        &#125;)</code></pre><p>同样的引用方式</p><pre><code class="lang-js">this.$route.params.id</code></pre><h3 id="router-push传参"><a href="#router-push传参" class="headerlink" title="$router.push传参"></a>$router.push传参</h3><p>父组件</p><pre><code class="lang-js">      getDescribe(id) &#123;//   直接调用$router.push 实现携带参数的跳转        this.$router.push(&#123;          path: `/describe/$&#123;id&#125;`,        &#125;)</code></pre><p>此时需要修改router中的index.js，需要在path中添加/:id来对应 $router.push 中path携带的参数</p><pre><code class="lang-js">&#123;     path: &#39;/describe/:id&#39;,     name: &#39;Describe&#39;,     component: Describe   &#125;</code></pre><p>同样的引用方式</p><pre><code class="lang-js">this.$route.params.id</code></pre>]]></content>
    
    
    <summary type="html">Vue项目路由传参方法</summary>
    
    
    
    <category term="Vue" scheme="https://yanjianzhang.github.io/categories/Vue/"/>
    
    
    <category term="Vue" scheme="https://yanjianzhang.github.io/tags/Vue/"/>
    
    <category term="router" scheme="https://yanjianzhang.github.io/tags/router/"/>
    
  </entry>
  
  <entry>
    <title>Hackerrank Dicts 解题记录</title>
    <link href="https://yanjianzhang.github.io/2018/08/30/dict_hr/"/>
    <id>https://yanjianzhang.github.io/2018/08/30/dict_hr/</id>
    <published>2018-08-30T18:44:02.000Z</published>
    <updated>2018-08-31T07:42:50.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Hash-Tables-Ransom-Note"><a href="#Hash-Tables-Ransom-Note" class="headerlink" title="Hash Tables: Ransom Note"></a><a href="https://www.hackerrank.com/challenges/ctci-ransom-note">Hash Tables: Ransom Note</a></h3><p>一道简易的题目，由于碰到了我经常用到的库 Counter，所以在此还是记录一下</p><pre><code class="lang-python">from collections import Counterdef checkMagazine(magazine, note):    magazineList =  Counter(magazine)    magazineDict = &#123;v:magazineList[v] for v in magazineList&#125;    noteList = Counter(note)    noteDict = &#123;v:noteList[v] for v in noteList&#125;    for v in noteDict:        if v not in magazineDict or magazineDict[v] &lt; noteDict[v]:            print(&quot;No&quot;)            return    print(&quot;Yes&quot;)    return</code></pre><p>对比一下其他人使用相同的库的代码，感觉别人写的实在是太骚了呀，深深感觉到自己的掌握还远远不够</p><pre><code class="lang-python">def checkMagazine(magazine, note):    if not (Counter(note) - Counter(magazine)):    # or Counter(note) - Counter(magazine) == &#123;&#125;        print(&quot;Yes&quot;)    else:        print(&quot;No&quot;)</code></pre><hr><h3 id="Sherlock-and-Anagrams"><a href="#Sherlock-and-Anagrams" class="headerlink" title="Sherlock and Anagrams"></a><a href="https://www.hackerrank.com/challenges/sherlock-and-anagrams">Sherlock and Anagrams</a></h3><p>一道中等难度的题目,最初采用将字符串都拆解成他们的不同字串的方法，结果超时了，想想也是，如果是100个字母长度的字符串，那么结果就是2<sup>100</sup>,不超时才怪。</p><pre><code class="lang-python">from collections import Counterdef checkAnag(a,b):    if Counter(a) == Counter(b):        return Truedef sherlockAndAnagrams(s):    count = 0    s1 = &quot;&quot;.join(reversed(s))    sub1 =[s1[i:i+x+1] for i in range(len(s1)) for x in range(len(s1)-i)]    if s1 in sub1:        sub1.remove(s1)    for i in range(len(sub1)):        for j in range(i+1,len(sub1)):            if checkAnag(sub1[i],sub1[j]):                count += 1    return count</code></pre><p>与其对其中的每一个字母进行计数，还不如将所有的字母都按顺序排列一遍，然后再比较最后的两个序列是否相同，此思路的实现代码如下</p><pre><code class="lang-python">def sherlockAndAnagrams(s):    count=0    dict=&#123;&#125;    n=len(s)    for i in range(n):        for j in range(n-i):            sub=&#39;&#39;.join(sorted(s[j:j+i+1]))            try:                dict[sub]+=1            except:                dict[sub]=1    for i in dict:        count+=dict[i]*(dict[i]-1)//2    return count</code></pre><hr><h3 id="Count-Triplets"><a href="#Count-Triplets" class="headerlink" title="Count Triplets"></a><a href="https://www.hackerrank.com/challenges/count-triplets-1">Count Triplets</a></h3><p>对三个数进行r倍组合，1为特殊情况，我直接用了组合计算，不过代码出现了超时，再研究研究哪里出了问题</p><pre><code class="lang-python">from collections import Counterfrom itertools import combinations# Complete the countTriplets function below.def combinationResult(num):    i = 0    for combination in combinations(range(num),3):        i += 1    return i def countTriplets(arr, r):    sum = 0    square = r**2    countNum = Counter(arr)    if r == 1:        for v in countNum:            if(countNum[v]&gt;2):                sum += combinationResult(countNum[v])        return sum    sortSet = list(sorted(set(arr)))    firstLimit = sortSet[len(sortSet)-1]/square    for x in sortSet:        if x &gt; firstLimit:            break        if x*r in sortSet and x*square in sortSet:            sum += countNum[x]*countNum[x*r]*countNum[x*square]    return sum</code></pre><p>关于组合数的计算，参见<a href="https://blog.csdn.net/weixin_41947081/article/details/80740756">python计算组合数的两种实现方法</a><br>经过对两种方法(直接进行组合与笛卡尔乘积)的比对，得出直接利用combinations来计算更加迅速一些,以下为比较代码：</p><pre><code class="lang-python">import mathimport itertoolsimport timefrom itertools import combinationsunique = [1, 2, 3, 4, 5, 56, 78, 23]time1 = time.time()for j in range(1000000):    i = 0    for combination in combinations(unique, 2):        i += 1time2 = time.time()for j in range(1000000):    d = 0    uniques = [unique, unique]    for combination in itertools.product(*uniques):        d += 1time3 = time.time()print(&quot;time 2-1 = &quot;+str(time2-time1), &quot;time 3-1 = &quot;+str(time3-time2))#time 2-1 = 3.7130393981933594 time 3-1 = 7.952743053436279</code></pre><p>为了将这道题解决，我看了一下出题者的解释，在出题者的hint中提到本题是能够达到O(n)的复杂度的，反观一下自己，虽然自己并没有写什么循环，不过的话在循环里的if语句中用到了两个in，基本算是O(n<sup>3</sup>)的复杂度了。<br>按照作者的意思，作者进行个对arr的遍历，建立两个字典，假设三元组为(A，B，C)，两个字典分别是指对于B来说A已经存在了，以及对于C来说（A，B）的组合已经准备好了。<br>每个字典里面放的并不是实值，而是对于未来三元组的一个可能性的预测，在遍历每个A的同时让B与C的线性空间不断减小，是一个非常有意思的解决思路。以下是它的python代码。</p><pre><code class="lang-python">def countTriplets(arr, r):    r2 = Counter()    r3 = Counter()    count = 0    for v in arr:        if v in r3:            count += r3[v]        if v in r2:            r3[v*r] += r2[v]        r2[v*r] += 1    return count</code></pre>]]></content>
    
    
    <summary type="html">对HackerRank Interview preparation中Dict组中的题目进行分析记录</summary>
    
    
    
    <category term="hackerrank" scheme="https://yanjianzhang.github.io/categories/hackerrank/"/>
    
    
    <category term="hackerrank" scheme="https://yanjianzhang.github.io/tags/hackerrank/"/>
    
  </entry>
  
</feed>
